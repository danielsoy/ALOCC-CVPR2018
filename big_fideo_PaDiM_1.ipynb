{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsoy/ALOCC-CVPR2018/blob/master/big_fideo_PaDiM_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ReQmXBbrEbc"
      },
      "source": [
        "Paper: https://arxiv.org/pdf/2011.08785.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "DdYkd7CVPEy4"
      },
      "outputs": [],
      "source": [
        "%pip install dotmap -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0Ey3gxRW6Cb"
      },
      "source": [
        "#### Explain dataset directory structure\n",
        "\n",
        "Here, we download 2 datasets: **MVTec-AD** (contains many different objects) and **Transistor** dataset for testing purpose\n",
        "\n",
        "**Transistor dataset should be replaced by your custom dataset.**\n",
        "\n",
        "Your custom dataset should have the following order:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "In the example above, `transistor` is the dataset root. It will contain a `train` folder and a `test` folder.\n",
        "\n",
        "In the `train` folder, it only contains `good` folder. Inside of `good` folder, there are normal images (OK).\n",
        "\n",
        "In the `test` folder, it contains `good` and other anomaly folders. Since, we don't care about these anomaly classes. We can specify only a `anomaly` folder, or whatever the name that you want. In short, `test` will contain `good` and `anomaly` folder. Inside of `good`, there are normal images. Inside of `anomaly` folder, there are abnormal (NG) images."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from random import sample\n",
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.covariance import LedoitWolf\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from skimage import morphology\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from dotmap import DotMap\n",
        "import yaml\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import wide_resnet50_2, resnet18, ResNet18_Weights, Wide_ResNet50_2_Weights\n",
        "from torchvision import transforms\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "zrLcj1Nm9c2b"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD9Br8QiGGxk"
      },
      "source": [
        "### Import, define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "58a8jN_BSsex"
      },
      "outputs": [],
      "source": [
        "def denormalization(x):\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    x = (((x.transpose(1, 2, 0) * std) + mean) * 255.).astype(np.uint8)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def embedding_concat(x, y):\n",
        "    B, C1, H1, W1 = x.size()\n",
        "    _, C2, H2, W2 = y.size()\n",
        "    s = int(H1 / H2)\n",
        "    x = F.unfold(x, kernel_size=s, dilation=1, stride=s)\n",
        "    x = x.view(B, C1, -1, H2, W2)\n",
        "    z = torch.zeros(B, C1 + C2, x.size(2), H2, W2).to(x.device)\n",
        "    for i in range(x.size(2)):\n",
        "        z[:, :, i, :, :] = torch.cat((x[:, :, i, :, :], y), 1)\n",
        "    z = z.view(B, -1, H2 * W2)\n",
        "    z = F.fold(z, kernel_size=s, output_size=(H1, W1), stride=s)\n",
        "\n",
        "    return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogaQkNcLW6Ce"
      },
      "source": [
        "#### Load train and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ljIhalqcSw5Q"
      },
      "outputs": [],
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, root, tfms=None):\n",
        "        super(TrainDataset, self).__init__()\n",
        "        self.tfms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize([224, 224], Image.ANTIALIAS),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                )\n",
        "            ]\n",
        "        ) if tfms is None else tfms\n",
        "\n",
        "        self.datas = self.read_data(root)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.datas[idx]).convert('RGB')\n",
        "        img = self.tfms(img)\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.datas)\n",
        "\n",
        "    def get_labels(self):\n",
        "        return np.arange(len(self.datas)).tolist()\n",
        "\n",
        "    def read_data(self, root):\n",
        "        imgs = glob.glob(os.path.join(root, 'train', 'good', '*'))\n",
        "        return imgs\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root, tfms=None):\n",
        "        super(TestDataset, self).__init__()\n",
        "        self.tfms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize([224, 224], Image.ANTIALIAS),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                )\n",
        "            ]\n",
        "        ) if tfms is None else tfms\n",
        "\n",
        "        self.datas = self.read_data(root)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.datas[idx][0]).convert('RGB')\n",
        "        img = self.tfms(img)\n",
        "        label = self.datas[idx][1]\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.datas)\n",
        "\n",
        "    def read_data(self, root):\n",
        "        ims, labels = [], []\n",
        "        cls_idx = None\n",
        "        for cls in os.listdir(os.path.join(root, 'test')):\n",
        "            im_paths = sorted(glob.glob(os.path.join(root, 'test', cls, '*')))\n",
        "            if cls == 'good': cls_idx = 0\n",
        "            else: cls_idx = 1\n",
        "            all_labels = [tuple([imp, cls_idx]) for imp in im_paths]\n",
        "            ims.extend(all_labels)\n",
        "            # if cls == 'good': labels.extend([0] * len(im_paths))\n",
        "            # else: labels.extend([1] * len(im_paths))\n",
        "        return ims"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0FBzs6bW6Cf"
      },
      "source": [
        "#### Padim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Llg7kCUg_xtr"
      },
      "outputs": [],
      "source": [
        "class PaDiM():\n",
        "    def __init__(self, args) -> None:\n",
        "        self.input_size = (args.MODEL.INPUT_SIZE, args.MODEL.INPUT_SIZE)\n",
        "        self.device = args.TRAIN.DEVICE\n",
        "        self.save_dir = args.TRAIN.SAVE_DIR\n",
        "        self.checkpoint_path = args.INFERENCE.CHECKPOINT_PATH\n",
        "\n",
        "        if args.TRAIN.PRETRAINED:\n",
        "            self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1).to(self.device)\n",
        "        else:\n",
        "            self.backbone = resnet18().to(self.device)\n",
        "\n",
        "        self.t_d = 448\n",
        "        # random pick features\n",
        "        self.d = args.INFERENCE.REDUCE_FEATURES\n",
        "        self.backbone.eval()\n",
        "        self.idx = torch.tensor(sample(range(0, self.t_d), self.d)).to(self.device)\n",
        "\n",
        "        self.output_layers = list()\n",
        "\n",
        "        self.backbone.layer1[-1].register_forward_hook(self.hooks)\n",
        "        self.backbone.layer2[-1].register_forward_hook(self.hooks)\n",
        "        self.backbone.layer3[-1].register_forward_hook(self.hooks)\n",
        "\n",
        "        self.distribution = None\n",
        "        self.threshold = None\n",
        "        self.max_score = None\n",
        "        self.min_score = None\n",
        "\n",
        "    def predict(self, data):\n",
        "        '''\n",
        "            data [torch.Tensor]: input data which has shape (batch, C, H, W)\n",
        "                H, W    = (224, 224)\n",
        "                C       = 3\n",
        "                batch   = 1\n",
        "        '''\n",
        "        self.backbone.eval()\n",
        "\n",
        "        outputs = OrderedDict([('layer1', []), ('layer2', []), ('layer3', [])])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _ = self.backbone(data.to(self.device))\n",
        "\n",
        "        for k, v in zip(outputs.keys(), self.output_layers):\n",
        "            outputs[k].append(v.detach())\n",
        "\n",
        "        self.output_layers.clear()\n",
        "\n",
        "        for k, v in outputs.items():\n",
        "            outputs[k] = torch.cat(v, 0)\n",
        "\n",
        "        # Embedding concat\n",
        "        embedding_vectors = outputs['layer1']\n",
        "        for layer_name in ['layer2', 'layer3']:\n",
        "            embedding_vectors = embedding_concat(embedding_vectors, outputs[layer_name])\n",
        "\n",
        "        # randomly select d dimension\n",
        "        embedding_vectors = torch.index_select(embedding_vectors, 1, self.idx)\n",
        "        # calculate distance matrix\n",
        "        B, C, H, W = embedding_vectors.size()\n",
        "        embedding_vectors = embedding_vectors.view(B, C, H * W)\n",
        "        dist_list = []\n",
        "        for i in range(H * W):\n",
        "            mean = self.distribution[0][:, i]\n",
        "            conv_inv = torch.linalg.inv(self.distribution[1][:, :, i])\n",
        "            dist = [self.mahalanobis_torch(sample[:, i], mean, conv_inv) for sample in embedding_vectors]\n",
        "            dist_list.append(dist)\n",
        "\n",
        "        dist_list = torch.tensor(dist_list).transpose(1, 0).reshape(B, H, W)\n",
        "\n",
        "        # upsample\n",
        "        score_map = F.interpolate(dist_list.unsqueeze(1), size=data.size(2), mode='bilinear',\n",
        "                                    align_corners=False).squeeze(1).numpy()\n",
        "\n",
        "        # apply gaussian smoothing on the score map\n",
        "        for i in range(score_map.shape[0]):\n",
        "            score_map[i] = gaussian_filter(score_map[i], sigma=4)\n",
        "\n",
        "        # Normalization\n",
        "        # self.max_score = score_map.max()\n",
        "        # self.min_score = score_map.min()\n",
        "        scores = (score_map - self.min_score) / (self.max_score - self.min_score)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def train(self, dataloader):\n",
        "        self.backbone.eval()\n",
        "        train_outputs = OrderedDict([('layer1', []), ('layer2', []), ('layer3', [])])\n",
        "\n",
        "        for img in dataloader:\n",
        "            img = img.to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _ = self.backbone(img)\n",
        "\n",
        "            for k, v in zip(train_outputs.keys(), self.output_layers):\n",
        "                train_outputs[k].append(v.detach())\n",
        "\n",
        "            self.output_layers.clear()\n",
        "\n",
        "        for k, v in train_outputs.items():\n",
        "            train_outputs[k] = torch.cat(v, 0)\n",
        "\n",
        "        # Embedding concat\n",
        "        embedding_vectors = train_outputs['layer1']\n",
        "        for layer_name in ['layer2', 'layer3']:\n",
        "            embedding_vectors = embedding_concat(embedding_vectors, train_outputs[layer_name])\n",
        "\n",
        "        # randomly select d dimension\n",
        "        embedding_vectors = torch.index_select(embedding_vectors, 1, self.idx)\n",
        "        # calculate multivariate Gaussian distribution\n",
        "        B, C, H, W = embedding_vectors.size()\n",
        "        embedding_vectors = embedding_vectors.view(B, C, H * W)\n",
        "        mean = torch.mean(embedding_vectors, dim=0)\n",
        "        cov = torch.zeros(C, C, H * W).to(self.device)\n",
        "        I = torch.eye(C).to(self.device)\n",
        "        for i in range(H * W):\n",
        "            # cov[:, :, i] = LedoitWolf().fit(embedding_vectors[:, :, i].numpy()).covariance_\n",
        "            cov[:, :, i] = torch.cov(embedding_vectors[:, :, i].T) + 0.01 * I\n",
        "        # save learned distribution\n",
        "        self.distribution = [mean, cov]\n",
        "\n",
        "    def evaluate(self, dataloader):\n",
        "        self.backbone.eval()\n",
        "        test_outputs = OrderedDict([('layer1', []), ('layer2', []), ('layer3', [])])\n",
        "\n",
        "        gt_list = list()\n",
        "\n",
        "        for img, label in dataloader:\n",
        "            img = img.to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _ = self.backbone(img)\n",
        "\n",
        "            for k, v in zip(test_outputs.keys(), self.output_layers):\n",
        "                test_outputs[k].append(v.detach())\n",
        "\n",
        "            gt_list.extend(label.cpu().detach().numpy())\n",
        "\n",
        "            self.output_layers.clear()\n",
        "\n",
        "        for k, v in test_outputs.items():\n",
        "            test_outputs[k] = torch.cat(v, 0)\n",
        "\n",
        "        # Embedding concat\n",
        "        embedding_vectors = test_outputs['layer1']\n",
        "        for layer_name in ['layer2', 'layer3']:\n",
        "            embedding_vectors = embedding_concat(embedding_vectors, test_outputs[layer_name])\n",
        "\n",
        "        # randomly select d dimension\n",
        "        embedding_vectors = torch.index_select(embedding_vectors, 1, self.idx)\n",
        "        # calculate distance matrix\n",
        "        B, C, H, W = embedding_vectors.size()\n",
        "        embedding_vectors = embedding_vectors.view(B, C, H * W)\n",
        "        dist_list = []\n",
        "        for i in range(H * W):\n",
        "            mean = self.distribution[0][:, i]\n",
        "            conv_inv = torch.linalg.inv(self.distribution[1][:, :, i])\n",
        "            dist = [self.mahalanobis_torch(sample[:, i], mean, conv_inv) for sample in embedding_vectors]\n",
        "            dist_list.append(dist)\n",
        "\n",
        "        dist_list = torch.tensor(dist_list).transpose(1, 0).reshape(B, H, W)\n",
        "\n",
        "        # upsample\n",
        "        score_map = F.interpolate(dist_list.unsqueeze(1), size=self.input_size[0], mode='bilinear',\n",
        "                                    align_corners=False).squeeze().numpy()\n",
        "\n",
        "        # apply gaussian smoothing on the score map\n",
        "        for i in range(score_map.shape[0]):\n",
        "            score_map[i] = gaussian_filter(score_map[i], sigma=4)\n",
        "\n",
        "        # Normalization\n",
        "        self.max_score = score_map.max()\n",
        "        self.min_score = score_map.min()\n",
        "        scores = (score_map - self.min_score) / (self.max_score - self.min_score)\n",
        "\n",
        "        # calculate image-level ROC AUC score\n",
        "        img_scores = scores.reshape(scores.shape[0], -1).max(axis=1)\n",
        "        gt_list = np.asarray(gt_list)\n",
        "        img_roc_auc = roc_auc_score(gt_list, img_scores)\n",
        "        # self.threshold = self.find_optimal_threshold(gt_list, img_scores)\n",
        "        # print(\"Optimal threshold is:\", self.threshold)\n",
        "        return img_roc_auc\n",
        "\n",
        "    def mahalanobis_torch(self, u, v, cov):\n",
        "        delta = u - v\n",
        "        m = torch.dot(delta, torch.matmul(cov, delta))\n",
        "        return torch.sqrt(m)\n",
        "\n",
        "    # def find_optimal_threshold(self, gt_list, img_scores):\n",
        "    #     precision, recall, thresholds = precision_recall_curve(gt_list, img_scores)\n",
        "    #     a = 2 * precision * recall\n",
        "    #     b = precision + recall\n",
        "    #     f1 = np.divide(a, b, out=np.zeros_like(a), where=b != 0)\n",
        "    #     threshold = thresholds[np.argmax(f1)]\n",
        "    #     return threshold\n",
        "\n",
        "    def save_checkpoint(self, filename):\n",
        "        ckp = {\n",
        "            \"max_score\": self.max_score,\n",
        "            \"min_score\": self.min_score,\n",
        "            \"threshold\": self.threshold,\n",
        "            \"idx\": self.idx,\n",
        "            \"dist\": self.distribution\n",
        "        }\n",
        "        path = os.path.join(self.save_dir, filename)\n",
        "        torch.save(ckp, path)\n",
        "        # with open(path, 'wb') as f:\n",
        "        #     pickle.dump(ckp, f)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        # with open(self.checkpoint_path, 'rb') as f:\n",
        "        #     ckp = pickle.load(f)\n",
        "        ckp = torch.load(self.checkpoint_path, map_location=self.device)\n",
        "        self.max_score = ckp['max_score']\n",
        "        self.min_score = ckp['min_score']\n",
        "        self.threshold = ckp['threshold']\n",
        "        self.idx = ckp['idx']\n",
        "        self.distribution = ckp['dist']\n",
        "\n",
        "    def hooks(self, module, input, output):\n",
        "        self.output_layers.append(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qi6_ylmGPPy"
      },
      "source": [
        "### Config\n",
        "\n",
        "INPUT_SIZE: size of image data\n",
        "\n",
        "EPOCH: number of epoch\n",
        "\n",
        "BATCH_SIZE: number of batchsize\n",
        "\n",
        "PRETRAINED (bool): use pretrained vgg to extract features\n",
        "\n",
        "DEVICE: \"cpu\" or \"cuda\"\n",
        "\n",
        "DATA_DIR: path to your dataset\n",
        "\n",
        "SAVE_DIR: path of directory to save checkpoint file (weight file)\n",
        "\n",
        "CHECKPOINT_PATH: path to checkpoint file so model can load and run inference\n",
        "\n",
        "REDUCE_FEATURE: reduce number of features to this number"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZZQ-ENpX_vo",
        "outputId": "51ca0afe-7153-43e1-f99a-7ef8edc049c4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UsC33sLAc4q",
        "outputId": "315eabc7-cf12-4875-fbeb-0ed4207cad20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/config.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/config.yaml\n",
        "MODEL:\n",
        "  INPUT_SIZE: 224\n",
        "TRAIN:\n",
        "  EPOCH: 5\n",
        "  BATCH_SIZE: 16\n",
        "  PRETRAINED: True\n",
        "  DEVICE: \"cpu\"\n",
        "\n",
        "\n",
        "  DATA_DIR: \"/content/drive/MyDrive/big_fideo\"\n",
        "  SAVE_DIR: \"/content/drive/MyDrive/big_fideo\"\n",
        "  SAVE_DIR: \"/content\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "INFERENCE:\n",
        "  CHECKPOINT_PATH: \"/content/train_transistor_.pt\"\n",
        "  REDUCE_FEATURES: 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "1JYdYoTTAum5"
      },
      "outputs": [],
      "source": [
        "def read_args(filename):\n",
        "    with open(filename) as f:\n",
        "        cf = yaml.safe_load(f)\n",
        "    f.close()\n",
        "    args = DotMap(cf)\n",
        "    return args\n",
        "\n",
        "args = read_args(\"/content/config.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDqLI9n0S9Cm",
        "outputId": "ef4f6d95-bcb5-4f26-db11-2da14207a1b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of training set:  1000\n",
            "Size of testing set:  2\n"
          ]
        }
      ],
      "source": [
        "tfms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize([224, 224], Image.ANTIALIAS),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                )\n",
        "            ]\n",
        ")\n",
        "\n",
        "train_set = TrainDataset(root=args.TRAIN.DATA_DIR, tfms=tfms)\n",
        "test_set = TestDataset(root=args.TRAIN.DATA_DIR, tfms=tfms)\n",
        "\n",
        "train_dl = DataLoader(train_set, batch_size=args.TRAIN.BATCH_SIZE, num_workers=os.cpu_count(),\n",
        "                      shuffle=True, pin_memory=True, drop_last=False)\n",
        "\n",
        "test_dl = DataLoader(test_set, batch_size=args.TRAIN.BATCH_SIZE, num_workers=os.cpu_count(),\n",
        "                     shuffle=False, pin_memory=True)\n",
        "\n",
        "print('Size of training set: ', len(train_set))\n",
        "print('Size of testing set: ', len(test_set))  # solo deje damaged case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "S2k-fpaOTd9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "663824b9-be5c-4dc4-8851-b4bb35539ea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 108MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = PaDiM(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj3xiPbZEMe1"
      },
      "outputs": [],
      "source": [
        "# train y salva modelo entrenado como train_transistor_0.92.pt\n",
        "model.train(train_dl)\n",
        "\n",
        "# test\n",
        "roc_auc = model.evaluate(test_dl)\n",
        "\n",
        "print(\"ROC AUC score:\", roc_auc)\n",
        "\n",
        "model.save_checkpoint(filename=f'train_transistor_{roc_auc:.2f}.pt')  # train_transistor_0.92.pt\n",
        "\n",
        "model.save_checkpoint(filename=f'train_transistor_.pt')  # train_transistor_.pt  roc_auc agnostico.\n",
        "# /content/drive/MyDrive/transistor/train_transistor_.pt\n",
        "\n",
        "print('Max score:', model.max_score, 'Min score', model.min_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_ytRThwm6zs"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSRLyQcftwdE"
      },
      "outputs": [],
      "source": [
        "test_model = PaDiM(args)\n",
        "test_model.load_checkpoint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuGZZAQ5Sri2"
      },
      "outputs": [],
      "source": [
        "threshold = 0.4\n",
        "\n",
        "path = '/content/drive/MyDrive/fideo/test/bad/0 (28).PNG'\n",
        "\n",
        "infer_tfms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize([224, 224]),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                )\n",
        "            ]\n",
        ")\n",
        "\n",
        "img_ = Image.open(path).convert('RGB')\n",
        "img = tfms(img_)\n",
        "img = img.unsqueeze(0)\n",
        "\n",
        "scores = test_model.predict(img)\n",
        "\n",
        "pred = np.any(scores > threshold)\n",
        "print(\"Anomaly: \", pred)\n",
        "\n",
        "mask = np.where(scores > threshold, 1, 0)\n",
        "\n",
        "vis_img = img_.resize((224, 224))\n",
        "\n",
        "f, axarr = plt.subplots(1,2, figsize=(15, 15))\n",
        "axarr[0].imshow(vis_img)\n",
        "axarr[1].imshow(mask.squeeze(0))\n",
        "axarr[1].imshow(vis_img, alpha = 0.75)\n",
        "# axarr[2].imshow(true_mask)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95Bf86UYxgmh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "openvino_env",
      "language": "python",
      "name": "openvino_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}