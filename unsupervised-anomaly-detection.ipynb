{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsoy/ALOCC-CVPR2018/blob/master/unsupervised-anomaly-detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17501f41",
      "metadata": {
        "id": "17501f41"
      },
      "source": [
        "# Unsupervised anomaly detection using Anomalib library"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ae2218",
      "metadata": {
        "id": "28ae2218"
      },
      "source": [
        "_________________________\n",
        "Author: Dennis Hernando NÚÑEZ FERNÁNDEZ    \n",
        "Author website: [https://dennishnf.com](https://dennishnf.com)   \n",
        "\n",
        "Modified by: Aditya Bhattacharya\n",
        "\n",
        "Collaborator website: [https://aditya-bhattacharya.net/](https://aditya-bhattacharya.net/)    \n",
        "_________________________    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f8595c0",
      "metadata": {
        "id": "2f8595c0"
      },
      "source": [
        "## Pre-installation settings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f44ca43",
      "metadata": {
        "id": "5f44ca43"
      },
      "source": [
        "All the experiments were runned on a machine with Ubuntu 20.04 LTS and NVIDIA GeForce GTX 1050 Ti with 4GB.    \n",
        "In addition, this notebook was locally implemented using Conda and in a virtual environment.     \n",
        "Therefore, to create a virtual enviroment in Conda, the following commands were introduced in the terminal:    \n",
        "\n",
        "```\n",
        "$ yes | conda create -n anomalib_env python=3.8\n",
        "$ conda activate anomalib_env\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7657b181",
      "metadata": {
        "id": "7657b181"
      },
      "source": [
        "## Installing AnomaLib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib==3.1.3 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEpEjVqy9WI2",
        "outputId": "c0b92553-b027-425a-9b46-3d2c5c9b8f0f"
      },
      "id": "PEpEjVqy9WI2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting matplotlib==3.1.3\n",
            "  Downloading matplotlib-3.1.3.tar.gz (40.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.1.3) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.1.3) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.1.3) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.1.3) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.1.3) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.16.0)\n",
            "Building wheels for collected packages: matplotlib\n",
            "  Building wheel for matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for matplotlib: filename=matplotlib-3.1.3-cp39-cp39-linux_x86_64.whl size=12062530 sha256=efb90391eee645a128b5b71aa3cfc4c6f9c01989fb101ca30b21ce8679cbce33\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/5f/33/d7b8943eba74fdfbd535c83cefcf366c25b0f9cb6424e763e7\n",
            "Successfully built matplotlib\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotnine 0.10.1 requires matplotlib>=3.5.0, but you have matplotlib 3.1.3 which is incompatible.\n",
            "pandas-profiling 3.2.0 requires matplotlib>=3.2.0, but you have matplotlib 3.1.3 which is incompatible.\n",
            "mizani 0.8.1 requires matplotlib>=3.5.0, but you have matplotlib 3.1.3 which is incompatible.\n",
            "arviz 0.15.1 requires matplotlib>=3.2, but you have matplotlib 3.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4b9d428e",
      "metadata": {
        "id": "4b9d428e"
      },
      "outputs": [],
      "source": [
        "PROJECT_PATH = '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b45d330d",
      "metadata": {
        "id": "b45d330d",
        "outputId": "aa0497ec-f1ee-410e-9caa-f6e31ea616f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd {PROJECT_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b5090ed0",
      "metadata": {
        "id": "b5090ed0",
        "outputId": "a30d881b-68b5-4386-cb2f-5833c1329a37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6389e0f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6389e0f7",
        "outputId": "a7e40113-3906-457a-d26d-0c29baa4d862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'anomalib'...\n",
            "remote: Enumerating objects: 27705, done.\u001b[K\n",
            "remote: Counting objects: 100% (920/920), done.\u001b[K\n",
            "remote: Compressing objects: 100% (644/644), done.\u001b[K\n",
            "remote: Total 27705 (delta 295), reused 838 (delta 258), pack-reused 26785\u001b[K\n",
            "Receiving objects: 100% (27705/27705), 1.50 GiB | 27.03 MiB/s, done.\n",
            "Resolving deltas: 100% (15240/15240), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/openvinotoolkit/anomalib.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "daeb8683",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daeb8683",
        "outputId": "7ac6a2ab-8011-4030-cef8-597d7714a7f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/anomalib\n"
          ]
        }
      ],
      "source": [
        "%cd anomalib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9a8938c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a8938c3",
        "outputId": "79101ea8-8fb2-48e5-c3f9-cbd9bd70080e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHANGELOG.md\t    LICENSE\t    requirements\t      tools\n",
            "CITATION.cff\t    MANIFEST.in     setup.py\t\t      tox.ini\n",
            "CODE_OF_CONDUCT.md  notebooks\t    src\n",
            "CONTRIBUTING.md     pyproject.toml  tests\n",
            "docs\t\t    README.md\t    third-party-programs.txt\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "42255aa4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42255aa4",
        "outputId": "86eb2991-6b20-4a6e-e975-4969d3ad1a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for anomalib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -e . -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "82bcbc97",
      "metadata": {
        "id": "82bcbc97"
      },
      "outputs": [],
      "source": [
        "import anomalib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP80Y31J3Dau",
        "outputId": "c716d44e-d093-44aa-a85b-324e47a20c67"
      },
      "id": "HP80Y31J3Dau",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1srH9Q23Ljz",
        "outputId": "ad67711d-947b-4b32-d1be-254e14f25c38"
      },
      "id": "t1srH9Q23Ljz",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anomalib  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eN7h8zJ4N5H",
        "outputId": "db4ed9d4-7f6a-4f2e-e414-cfb2cdfc844a"
      },
      "id": "4eN7h8zJ4N5H",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: curl [options...] <url>\n",
            "     --abstract-unix-socket <path> Connect via abstract Unix domain socket\n",
            "     --alt-svc <file name> Enable alt-svc with this cache file\n",
            "     --anyauth       Pick any authentication method\n",
            " -a, --append        Append to target file when uploading\n",
            "     --basic         Use HTTP Basic Authentication\n",
            "     --cacert <file> CA certificate to verify peer against\n",
            "     --capath <dir>  CA directory to verify peer against\n",
            " -E, --cert <certificate[:password]> Client certificate file and password\n",
            "     --cert-status   Verify the status of the server certificate\n",
            "     --cert-type <type> Certificate file type (DER/PEM/ENG)\n",
            "     --ciphers <list of ciphers> SSL ciphers to use\n",
            "     --compressed    Request compressed response\n",
            "     --compressed-ssh Enable SSH compression\n",
            " -K, --config <file> Read config from a file\n",
            "     --connect-timeout <seconds> Maximum time allowed for connection\n",
            "     --connect-to <HOST1:PORT1:HOST2:PORT2> Connect to host\n",
            " -C, --continue-at <offset> Resumed transfer offset\n",
            " -b, --cookie <data|filename> Send cookies from string/file\n",
            " -c, --cookie-jar <filename> Write cookies to <filename> after operation\n",
            "     --create-dirs   Create necessary local directory hierarchy\n",
            "     --crlf          Convert LF to CRLF in upload\n",
            "     --crlfile <file> Get a CRL list in PEM format from the given file\n",
            " -d, --data <data>   HTTP POST data\n",
            "     --data-ascii <data> HTTP POST ASCII data\n",
            "     --data-binary <data> HTTP POST binary data\n",
            "     --data-raw <data> HTTP POST data, '@' allowed\n",
            "     --data-urlencode <data> HTTP POST data url encoded\n",
            "     --delegation <LEVEL> GSS-API delegation permission\n",
            "     --digest        Use HTTP Digest Authentication\n",
            " -q, --disable       Disable .curlrc\n",
            "     --disable-eprt  Inhibit using EPRT or LPRT\n",
            "     --disable-epsv  Inhibit using EPSV\n",
            "     --disallow-username-in-url Disallow username in url\n",
            "     --dns-interface <interface> Interface to use for DNS requests\n",
            "     --dns-ipv4-addr <address> IPv4 address to use for DNS requests\n",
            "     --dns-ipv6-addr <address> IPv6 address to use for DNS requests\n",
            "     --dns-servers <addresses> DNS server addrs to use\n",
            "     --doh-url <URL> Resolve host names over DOH\n",
            " -D, --dump-header <filename> Write the received headers to <filename>\n",
            "     --egd-file <file> EGD socket path for random data\n",
            "     --engine <name> Crypto engine to use\n",
            "     --etag-save <file> Get an ETag from response header and save it to a FILE\n",
            "     --etag-compare <file> Get an ETag from a file and send a conditional request\n",
            "     --expect100-timeout <seconds> How long to wait for 100-continue\n",
            " -f, --fail          Fail silently (no output at all) on HTTP errors\n",
            "     --fail-early    Fail on first transfer error, do not continue\n",
            "     --false-start   Enable TLS False Start\n",
            " -F, --form <name=content> Specify multipart MIME data\n",
            "     --form-string <name=string> Specify multipart MIME data\n",
            "     --ftp-account <data> Account data string\n",
            "     --ftp-alternative-to-user <command> String to replace USER [name]\n",
            "     --ftp-create-dirs Create the remote dirs if not present\n",
            "     --ftp-method <method> Control CWD usage\n",
            "     --ftp-pasv      Use PASV/EPSV instead of PORT\n",
            " -P, --ftp-port <address> Use PORT instead of PASV\n",
            "     --ftp-pret      Send PRET before PASV\n",
            "     --ftp-skip-pasv-ip Skip the IP address for PASV\n",
            "     --ftp-ssl-ccc   Send CCC after authenticating\n",
            "     --ftp-ssl-ccc-mode <active/passive> Set CCC mode\n",
            "     --ftp-ssl-control Require SSL/TLS for FTP login, clear for transfer\n",
            " -G, --get           Put the post data in the URL and use GET\n",
            " -g, --globoff       Disable URL sequences and ranges using {} and []\n",
            "     --happy-eyeballs-timeout-ms <milliseconds> How long to wait in milliseconds for IPv6 before trying IPv4\n",
            "     --haproxy-protocol Send HAProxy PROXY protocol v1 header\n",
            " -I, --head          Show document info only\n",
            " -H, --header <header/@file> Pass custom header(s) to server\n",
            " -h, --help          This help text\n",
            "     --hostpubmd5 <md5> Acceptable MD5 hash of the host public key\n",
            "     --http0.9       Allow HTTP 0.9 responses\n",
            " -0, --http1.0       Use HTTP 1.0\n",
            "     --http1.1       Use HTTP 1.1\n",
            "     --http2         Use HTTP 2\n",
            "     --http2-prior-knowledge Use HTTP 2 without HTTP/1.1 Upgrade\n",
            "     --http3         Use HTTP v3\n",
            "     --ignore-content-length Ignore the size of the remote resource\n",
            " -i, --include       Include protocol response headers in the output\n",
            " -k, --insecure      Allow insecure server connections when using SSL\n",
            "     --interface <name> Use network INTERFACE (or address)\n",
            " -4, --ipv4          Resolve names to IPv4 addresses\n",
            " -6, --ipv6          Resolve names to IPv6 addresses\n",
            " -j, --junk-session-cookies Ignore session cookies read from file\n",
            "     --keepalive-time <seconds> Interval time for keepalive probes\n",
            "     --key <key>     Private key file name\n",
            "     --key-type <type> Private key file type (DER/PEM/ENG)\n",
            "     --krb <level>   Enable Kerberos with security <level>\n",
            "     --libcurl <file> Dump libcurl equivalent code of this command line\n",
            "     --limit-rate <speed> Limit transfer speed to RATE\n",
            " -l, --list-only     List only mode\n",
            "     --local-port <num/range> Force use of RANGE for local port numbers\n",
            " -L, --location      Follow redirects\n",
            "     --location-trusted Like --location, and send auth to other hosts\n",
            "     --login-options <options> Server login options\n",
            "     --mail-auth <address> Originator address of the original email\n",
            "     --mail-from <address> Mail from this address\n",
            "     --mail-rcpt <address> Mail to this address\n",
            " -M, --manual        Display the full manual\n",
            "     --max-filesize <bytes> Maximum file size to download\n",
            "     --max-redirs <num> Maximum number of redirects allowed\n",
            " -m, --max-time <seconds> Maximum time allowed for the transfer\n",
            "     --metalink      Process given URLs as metalink XML file\n",
            "     --negotiate     Use HTTP Negotiate (SPNEGO) authentication\n",
            " -n, --netrc         Must read .netrc for user name and password\n",
            "     --netrc-file <filename> Specify FILE for netrc\n",
            "     --netrc-optional Use either .netrc or URL\n",
            " -:, --next          Make next URL use its separate set of options\n",
            "     --no-alpn       Disable the ALPN TLS extension\n",
            " -N, --no-buffer     Disable buffering of the output stream\n",
            "     --no-keepalive  Disable TCP keepalive on the connection\n",
            "     --no-npn        Disable the NPN TLS extension\n",
            "     --no-progress-meter Do not show the progress meter\n",
            "     --no-sessionid  Disable SSL session-ID reusing\n",
            "     --noproxy <no-proxy-list> List of hosts which do not use proxy\n",
            "     --ntlm          Use HTTP NTLM authentication\n",
            "     --ntlm-wb       Use HTTP NTLM authentication with winbind\n",
            "     --oauth2-bearer <token> OAuth 2 Bearer Token\n",
            " -o, --output <file> Write to file instead of stdout\n",
            " -Z, --parallel      Perform transfers in parallel\n",
            "     --parallel-immediate Do not wait for multiplexing (with --parallel)\n",
            "     --parallel-max  Maximum concurrency for parallel transfers\n",
            "     --pass <phrase> Pass phrase for the private key\n",
            "     --path-as-is    Do not squash .. sequences in URL path\n",
            "     --pinnedpubkey <hashes> FILE/HASHES Public key to verify peer against\n",
            "     --post301       Do not switch to GET after following a 301\n",
            "     --post302       Do not switch to GET after following a 302\n",
            "     --post303       Do not switch to GET after following a 303\n",
            "     --preproxy [protocol://]host[:port] Use this proxy first\n",
            " -#, --progress-bar  Display transfer progress as a bar\n",
            "     --proto <protocols> Enable/disable PROTOCOLS\n",
            "     --proto-default <protocol> Use PROTOCOL for any URL missing a scheme\n",
            "     --proto-redir <protocols> Enable/disable PROTOCOLS on redirect\n",
            " -x, --proxy [protocol://]host[:port] Use this proxy\n",
            "     --proxy-anyauth Pick any proxy authentication method\n",
            "     --proxy-basic   Use Basic authentication on the proxy\n",
            "     --proxy-cacert <file> CA certificate to verify peer against for proxy\n",
            "     --proxy-capath <dir> CA directory to verify peer against for proxy\n",
            "     --proxy-cert <cert[:passwd]> Set client certificate for proxy\n",
            "     --proxy-cert-type <type> Client certificate type for HTTPS proxy\n",
            "     --proxy-ciphers <list> SSL ciphers to use for proxy\n",
            "     --proxy-crlfile <file> Set a CRL list for proxy\n",
            "     --proxy-digest  Use Digest authentication on the proxy\n",
            "     --proxy-header <header/@file> Pass custom header(s) to proxy\n",
            "     --proxy-insecure Do HTTPS proxy connections without verifying the proxy\n",
            "     --proxy-key <key> Private key for HTTPS proxy\n",
            "     --proxy-key-type <type> Private key file type for proxy\n",
            "     --proxy-negotiate Use HTTP Negotiate (SPNEGO) authentication on the proxy\n",
            "     --proxy-ntlm    Use NTLM authentication on the proxy\n",
            "     --proxy-pass <phrase> Pass phrase for the private key for HTTPS proxy\n",
            "     --proxy-pinnedpubkey <hashes> FILE/HASHES public key to verify proxy with\n",
            "     --proxy-service-name <name> SPNEGO proxy service name\n",
            "     --proxy-ssl-allow-beast Allow security flaw for interop for HTTPS proxy\n",
            "     --proxy-tls13-ciphers <list> TLS 1.3 ciphersuites for proxy (OpenSSL)\n",
            "     --proxy-tlsauthtype <type> TLS authentication type for HTTPS proxy\n",
            "     --proxy-tlspassword <string> TLS password for HTTPS proxy\n",
            "     --proxy-tlsuser <name> TLS username for HTTPS proxy\n",
            "     --proxy-tlsv1   Use TLSv1 for HTTPS proxy\n",
            " -U, --proxy-user <user:password> Proxy user and password\n",
            "     --proxy1.0 <host[:port]> Use HTTP/1.0 proxy on given port\n",
            " -p, --proxytunnel   Operate through an HTTP proxy tunnel (using CONNECT)\n",
            "     --pubkey <key>  SSH Public key file name\n",
            " -Q, --quote         Send command(s) to server before transfer\n",
            "     --random-file <file> File for reading random data from\n",
            " -r, --range <range> Retrieve only the bytes within RANGE\n",
            "     --raw           Do HTTP \"raw\"; no transfer decoding\n",
            " -e, --referer <URL> Referrer URL\n",
            " -J, --remote-header-name Use the header-provided filename\n",
            " -O, --remote-name   Write output to a file named as the remote file\n",
            "     --remote-name-all Use the remote file name for all URLs\n",
            " -R, --remote-time   Set the remote file's time on the local output\n",
            " -X, --request <command> Specify request command to use\n",
            "     --request-target Specify the target for this request\n",
            "     --resolve <host:port:address[,address]...> Resolve the host+port to this address\n",
            "     --retry <num>   Retry request if transient problems occur\n",
            "     --retry-connrefused Retry on connection refused (use with --retry)\n",
            "     --retry-delay <seconds> Wait time between retries\n",
            "     --retry-max-time <seconds> Retry only within this period\n",
            "     --sasl-authzid <identity>  Use this identity to act as during SASL PLAIN authentication\n",
            "     --sasl-ir       Enable initial response in SASL authentication\n",
            "     --service-name <name> SPNEGO service name\n",
            " -S, --show-error    Show error even when -s is used\n",
            " -s, --silent        Silent mode\n",
            "     --socks4 <host[:port]> SOCKS4 proxy on given host + port\n",
            "     --socks4a <host[:port]> SOCKS4a proxy on given host + port\n",
            "     --socks5 <host[:port]> SOCKS5 proxy on given host + port\n",
            "     --socks5-basic  Enable username/password auth for SOCKS5 proxies\n",
            "     --socks5-gssapi Enable GSS-API auth for SOCKS5 proxies\n",
            "     --socks5-gssapi-nec Compatibility with NEC SOCKS5 server\n",
            "     --socks5-gssapi-service <name> SOCKS5 proxy service name for GSS-API\n",
            "     --socks5-hostname <host[:port]> SOCKS5 proxy, pass host name to proxy\n",
            " -Y, --speed-limit <speed> Stop transfers slower than this\n",
            " -y, --speed-time <seconds> Trigger 'speed-limit' abort after this time\n",
            "     --ssl           Try SSL/TLS\n",
            "     --ssl-allow-beast Allow security flaw to improve interop\n",
            "     --ssl-no-revoke Disable cert revocation checks (Schannel)\n",
            "     --ssl-reqd      Require SSL/TLS\n",
            " -2, --sslv2         Use SSLv2\n",
            " -3, --sslv3         Use SSLv3\n",
            "     --stderr        Where to redirect stderr\n",
            "     --styled-output Enable styled output for HTTP headers\n",
            "     --suppress-connect-headers Suppress proxy CONNECT response headers\n",
            "     --tcp-fastopen  Use TCP Fast Open\n",
            "     --tcp-nodelay   Use the TCP_NODELAY option\n",
            " -t, --telnet-option <opt=val> Set telnet option\n",
            "     --tftp-blksize <value> Set TFTP BLKSIZE option\n",
            "     --tftp-no-options Do not send any TFTP options\n",
            " -z, --time-cond <time> Transfer based on a time condition\n",
            "     --tls-max <VERSION> Set maximum allowed TLS version\n",
            "     --tls13-ciphers <list> TLS 1.3 ciphersuites (OpenSSL)\n",
            "     --tlsauthtype <type> TLS authentication type\n",
            "     --tlspassword   TLS password\n",
            "     --tlsuser <name> TLS user name\n",
            " -1, --tlsv1         Use TLSv1.0 or greater\n",
            "     --tlsv1.0       Use TLSv1.0 or greater\n",
            "     --tlsv1.1       Use TLSv1.1 or greater\n",
            "     --tlsv1.2       Use TLSv1.2 or greater\n",
            "     --tlsv1.3       Use TLSv1.3 or greater\n",
            "     --tr-encoding   Request compressed transfer encoding\n",
            "     --trace <file>  Write a debug trace to FILE\n",
            "     --trace-ascii <file> Like --trace, but without hex output\n",
            "     --trace-time    Add time stamps to trace/verbose output\n",
            "     --unix-socket <path> Connect through this Unix domain socket\n",
            " -T, --upload-file <file> Transfer local FILE to destination\n",
            "     --url <url>     URL to work with\n",
            " -B, --use-ascii     Use ASCII/text transfer\n",
            " -u, --user <user:password> Server user and password\n",
            " -A, --user-agent <name> Send User-Agent <name> to server\n",
            " -v, --verbose       Make the operation more talkative\n",
            " -V, --version       Show version number and quit\n",
            " -w, --write-out <format> Use output FORMAT after completion\n",
            "     --xattr         Store metadata in extended file attributes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! curl https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937637-1629952063/metal_nut.tar.xz --output metal_nut.tar.xz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTpRQ5_t392C",
        "outputId": "df0559d0-3970-49d6-d605-f99443df8ebc"
      },
      "id": "bTpRQ5_t392C",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  157M  100  157M    0     0  12.7M      0  0:00:12  0:00:12 --:--:-- 15.3M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf metal_nut.tar.xz"
      ],
      "metadata": {
        "id": "erAnirSm4r0Y"
      },
      "id": "erAnirSm4r0Y",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9dd43ca9",
      "metadata": {
        "id": "9dd43ca9"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b0748494",
      "metadata": {
        "id": "b0748494"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, pprint, yaml, warnings, math, glob, cv2, random, logging\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4964d882",
      "metadata": {
        "id": "4964d882"
      },
      "outputs": [],
      "source": [
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')\n",
        "logger = logging.getLogger(\"anomalib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "0a2688f1",
      "metadata": {
        "id": "0a2688f1",
        "outputId": "9e84a6e0-a317-4583-de3f-425769a84f3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-b4a6926e31e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manomalib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_everything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0manomalib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_configurable_parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0manomalib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_datamodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0manomalib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'anomalib.config'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import anomalib\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "from anomalib.config import get_configurable_parameters\n",
        "from anomalib.data import get_datamodule\n",
        "from anomalib.models import get_model\n",
        "from anomalib.utils.callbacks import LoadModelCallback, get_callbacks\n",
        "from anomalib.utils.loggers import configure_logger, get_experiment_logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba9d46fa",
      "metadata": {
        "id": "ba9d46fa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"GPU availability:\", torch.cuda.is_available())\n",
        "print(\"Number of GPU devices:\", torch.cuda.device_count())      \n",
        "print(\"Name of current GPU:\", torch.cuda.get_device_name(0))  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c76a0e04",
      "metadata": {
        "id": "c76a0e04"
      },
      "source": [
        "## Checking the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0829e7f",
      "metadata": {
        "id": "c0829e7f"
      },
      "outputs": [],
      "source": [
        "def list_files(startpath):\n",
        "    for root, dirs, files in os.walk(startpath):\n",
        "        level = root.replace(startpath, '').count(os.sep)\n",
        "        indent = ' ' * 4 * (level)\n",
        "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "        subindent = ' ' * 4 * (level + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d02c9d",
      "metadata": {
        "id": "64d02c9d"
      },
      "outputs": [],
      "source": [
        "#%cd {PROJECT_PATH}/dataset/MVTec/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3121716a",
      "metadata": {
        "id": "3121716a"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6508eb3a",
      "metadata": {
        "scrolled": false,
        "id": "6508eb3a"
      },
      "outputs": [],
      "source": [
        "list_files(\"metal_nut\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47ce8638",
      "metadata": {
        "id": "47ce8638"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2238f082",
      "metadata": {
        "id": "2238f082"
      },
      "source": [
        "### PaDiM: A Patch Distribution Modeling Framework for Anomaly Detection and Localization\n",
        "\n",
        "Paper: [PaDiM](https://arxiv.org/pdf/2011.08785.pdf)\n",
        "\n",
        "PaDiM is a patch based algorithm. It relies on a pre-trained CNN feature extractor. The image is broken into patches and embeddings are extracted from each patch using different layers of the feature extractors. The activation vectors from different layers are concatenated to get embedding vectors carrying information from different semantic levels and resolutions. This helps encode fine grained and global contexts. However, since the generated embedding vectors may carry redundant information, dimensions are reduced using random selection. A multivariate gaussian distribution is generated for each patch embedding across the entire training batch. Thus, for each patch of the set of training images, we have a different multivariate gaussian distribution. These gaussian distributions are represented as a matrix of gaussian parameters.\n",
        "\n",
        "During inference, Mahalanobis distance is used to score each patch position of the test image. It uses the inverse of the covariance matrix calculated for the patch during training. The matrix of Mahalanobis distances forms the anomaly map with higher scores indicating anomalous regions.\n",
        "\n",
        "![PaDiM Architecture](https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/docs/source/images/padim/architecture.jpg \"PaDiM Architecture\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da8e05f8",
      "metadata": {
        "id": "da8e05f8"
      },
      "source": [
        "### PatchCore\n",
        "\n",
        "Paper: [PatchCore](https://arxiv.org/pdf/2106.08265.pdf)\n",
        "\n",
        "The PatchCore algorithm is based on the idea that an image can be classified as anomalous as soon as a single patch is anomalous. The input image is tiled. These tiles act as patches which are fed into the neural network. It consists of a single pre-trained network which is used to extract \"mid\" level features patches. The \"mid\" level here refers to the feature extraction layer of the neural network model. Lower level features are generally too broad and higher level features are specific to the dataset the model is trained on. The features extracted during training phase are stored in a memory bank of neighbourhood aware patch level features.\n",
        "\n",
        "During inference this memory bank is coreset subsampled. Coreset subsampling generates a subset which best approximates the structure of the available set and allows for approximate solution finding. This subset helps reduce the search cost associated with nearest neighbour search. The anomaly score is taken as the maximum distance between the test patch in the test patch collection to each respective nearest neighbour.\n",
        "\n",
        "![PatchCore Architecture](https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/docs/source/images/patchcore/architecture.jpg \"PatchCore Architecture\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f114e940",
      "metadata": {
        "id": "f114e940"
      },
      "source": [
        "### Student-Teacher Feature Pyramid Matching for Unsupervised Anomaly Detection\n",
        "\n",
        "Paper: [STFPM](https://arxiv.org/pdf/2103.04257.pdf)\n",
        "\n",
        "STFPM algorithm consists of a pre-trained teacher network and a student network with identical architecture. The student network learns the distribution of anomaly-free images by matching the features with the counterpart features in the teacher network. Multi-scale feature matching is used to enhance robustness. This hierarchical feature matching enables the student network to receive a mixture of multi-level knowledge from the feature pyramid thus allowing for anomaly detection of various sizes.\n",
        "\n",
        "During inference, the feature pyramids of teacher and student networks are compared. Larger difference indicates a higher probability of anomaly occurrence.\n",
        "\n",
        "![STFPM Architecture](https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/docs/source/images/stfpm/architecture.jpg \"STFPM Architecture\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a373172d",
      "metadata": {
        "id": "a373172d"
      },
      "source": [
        "### FastFlow: Unsupervised Anomaly Detection and Localization via 2D Normalizing Flows\n",
        "\n",
        "Paper: [FastFlow](https://arxiv.org/abs/2111.07677)\n",
        "\n",
        "FastFlow is a two-dimensional normalizing flow-based probability distribution estimator. It can be used as a plug-in module with any deep feature extractor, such as ResNet and vision transformer, for unsupervised anomaly detection and localisation. In the training phase, FastFlow learns to transform the input visual feature into a tractable distribution, and in the inference phase, it assesses the likelihood of identifying anomalies.\n",
        "\n",
        "![FastFlow Architecture](https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/docs/source/images/fastflow/architecture.jpg \"FastFlow Architecture\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e09a03a",
      "metadata": {
        "id": "9e09a03a"
      },
      "source": [
        "### Anomaly Detection via Reverse Distillation from One-Class Embedding\n",
        "\n",
        "Paper: [Reverse Distillation](https://arxiv.org/pdf/2201.10703v2.pdf)\n",
        "\n",
        "Reverse Distillation model consists of three networks. The first is a pre-trained feature extractor (E). The next two are the one-class bottleneck embedding (OCBE) and the student decoder network (D). The backbone E is a ResNet model pre-trained on ImageNet dataset. During the forward pass, features from three ResNet block are extracted. These features are encoded by concatenating the three feature maps using the multi-scale feature fusion block of OCBE and passed to the decoder D. The decoder network is symmetrical to the feature extractor but reversed. During training, outputs from these symmetrical blocks are forced to be similar to the corresponding feature extractor layers by using cosine distance as the loss metric.\n",
        "\n",
        "During testing, a similar step is followed but this time the cosine distance between the feature maps is used to indicate the presence of anomalies. The distance maps from all the three layers are up-sampled to the image size and added (or multiplied) to produce the final feature map. Gaussian blur is applied to the output map to make it smoother. Finally, the anomaly map is generated by applying min-max normalization on the output map.\n",
        "\n",
        "![Anomaly Detection via Reverse Distillation from One-Class Embedding Architecture](https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/docs/source/images/reverse_distillation/architecture.png \"Reverse Distillation Architecture\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16ab19c0",
      "metadata": {
        "id": "16ab19c0"
      },
      "source": [
        "## Setting config files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb2e958f",
      "metadata": {
        "id": "eb2e958f"
      },
      "outputs": [],
      "source": [
        "%cd {'/content'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09e3e9e9",
      "metadata": {
        "id": "09e3e9e9"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1d2264",
      "metadata": {
        "id": "9d1d2264"
      },
      "outputs": [],
      "source": [
        "CONFIG_PATHS = '/content' + '/anomalib/anomalib/models'\n",
        "MODEL_CONFIG_PAIRS = {\n",
        "    'patchcore': f'{CONFIG_PATHS}/patchcore/config.yaml',\n",
        "    'padim':     f'{CONFIG_PATHS}/padim/config.yaml',\n",
        "    'cflow':     f'{CONFIG_PATHS}/cflow/config.yaml',\n",
        "    'dfkde':     f'{CONFIG_PATHS}/dfkde/config.yaml',\n",
        "    'dfm':       f'{CONFIG_PATHS}/dfm/config.yaml',\n",
        "    'ganomaly':  f'{CONFIG_PATHS}/ganomaly/config.yaml',\n",
        "    'stfpm':     f'{CONFIG_PATHS}/stfpm/config.yaml',\n",
        "    'fastflow':  f'{CONFIG_PATHS}/fastflow/config.yaml',\n",
        "    'draem':     f'{CONFIG_PATHS}/draem/config.yaml',\n",
        "    'reverse_distillation': f'{CONFIG_PATHS}/reverse_distillation/config.yaml',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e6bf4a4",
      "metadata": {
        "scrolled": false,
        "id": "6e6bf4a4"
      },
      "outputs": [],
      "source": [
        "MODEL = 'reverse_distillation'\n",
        "print(open(os.path.join(MODEL_CONFIG_PAIRS[MODEL]), 'r').read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9bd1686",
      "metadata": {
        "id": "b9bd1686"
      },
      "outputs": [],
      "source": [
        "new_update = {\n",
        "    \"path\": '/content',\n",
        "    'task': 'segmentation',\n",
        "    'category': 'metal_nut', \n",
        "    'image_size': 256,\n",
        "    'train_batch_size': 4,\n",
        "    'test_batch_size': 4,\n",
        "    'max_epochs': 4,\n",
        "    'seed': 101\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca380eaa",
      "metadata": {
        "id": "ca380eaa"
      },
      "outputs": [],
      "source": [
        "# update yaml key's value\n",
        "def update_yaml(old_yaml, new_yaml, new_update):\n",
        "    # load yaml\n",
        "    with open(old_yaml) as f:\n",
        "        old = yaml.safe_load(f)\n",
        "                  \n",
        "    temp = []\n",
        "    def set_state(old, key, value):\n",
        "        if isinstance(old, dict):\n",
        "            for k, v in old.items():\n",
        "                if k == 'project':\n",
        "                    temp.append(k)\n",
        "                if k == key:\n",
        "                    if temp and k == 'path':\n",
        "                        # right now, we don't wanna change `project.path`\n",
        "                        continue\n",
        "                    old[k] = value\n",
        "                elif isinstance(v, dict):\n",
        "                    set_state(v, key, value)\n",
        "    \n",
        "    # iterate over the new update key-value pari\n",
        "    for key, value in new_update.items():\n",
        "        set_state(old, key, value)\n",
        "    \n",
        "    # save the updated / modified yaml file\n",
        "    with open(new_yaml, 'w') as f:\n",
        "        yaml.safe_dump(old, f, default_flow_style=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00dcd180",
      "metadata": {
        "id": "00dcd180"
      },
      "outputs": [],
      "source": [
        "# let's set a new path location of new config file \n",
        "new_yaml_path = CONFIG_PATHS + '/' + MODEL + '_new.yaml'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79ecbbf8",
      "metadata": {
        "id": "79ecbbf8"
      },
      "outputs": [],
      "source": [
        "new_yaml_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93864b11",
      "metadata": {
        "id": "93864b11"
      },
      "outputs": [],
      "source": [
        "# run the update yaml method to update desired key's values\n",
        "update_yaml(MODEL_CONFIG_PAIRS[MODEL], new_yaml_path, new_update)        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c7a5e54",
      "metadata": {
        "scrolled": false,
        "id": "3c7a5e54"
      },
      "outputs": [],
      "source": [
        "with open(new_yaml_path) as f:\n",
        "    updated_config = yaml.safe_load(f)\n",
        "pprint.pprint(updated_config) # check if it's updated"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b5adf26",
      "metadata": {
        "id": "4b5adf26"
      },
      "source": [
        "##  Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f13b0bb",
      "metadata": {
        "id": "2f13b0bb"
      },
      "outputs": [],
      "source": [
        "if updated_config['project']['seed'] != 0:\n",
        "    print(updated_config['project']['seed'])\n",
        "    seed_everything(updated_config['project']['seed'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b794dbe",
      "metadata": {
        "id": "5b794dbe"
      },
      "outputs": [],
      "source": [
        "# It will return the configurable parameters in DictConfig object.\n",
        "config = get_configurable_parameters(\n",
        "    model_name=updated_config['model']['name'],\n",
        "    config_path=new_yaml_path\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d11707",
      "metadata": {
        "id": "41d11707"
      },
      "outputs": [],
      "source": [
        "# pass the config file to model, logger, callbacks and datamodule\n",
        "model      = get_model(config)\n",
        "experiment_logger = get_experiment_logger(config)\n",
        "callbacks  = get_callbacks(config)\n",
        "datamodule = get_datamodule(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f73c7c",
      "metadata": {
        "scrolled": false,
        "id": "28f73c7c"
      },
      "outputs": [],
      "source": [
        "# start training\n",
        "trainer = Trainer(**config.trainer, logger=experiment_logger, callbacks=callbacks)\n",
        "trainer.fit(model=model, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c863756b",
      "metadata": {
        "id": "c863756b"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1174f0ac",
      "metadata": {
        "scrolled": false,
        "id": "1174f0ac"
      },
      "outputs": [],
      "source": [
        "# load best model from checkpoint before evaluating\n",
        "load_model_callback = LoadModelCallback(\n",
        "    weights_path=trainer.checkpoint_callback.best_model_path\n",
        ")\n",
        "trainer.callbacks.insert(0, load_model_callback)\n",
        "trainer.test(model=model, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34bc81e",
      "metadata": {
        "id": "a34bc81e"
      },
      "source": [
        "## Visualization of the prediction on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b6d753d",
      "metadata": {
        "id": "5b6d753d"
      },
      "outputs": [],
      "source": [
        "RESULT_PATH = os.path.join(\n",
        "    updated_config['project']['path'],\n",
        "    updated_config['model']['name'],\n",
        "    updated_config['dataset']['format'], \n",
        "    updated_config['dataset']['category']\n",
        ")\n",
        "RESULT_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "368f5673",
      "metadata": {
        "id": "368f5673"
      },
      "outputs": [],
      "source": [
        "# a simple function to visualize the model's prediction (anomaly heatmap)\n",
        "def visualiz(paths, n_images, is_random=True, figsize=(16, 16)):\n",
        "    for i in range(n_images):\n",
        "        image_name = paths[i]\n",
        "        if is_random: image_name = random.choice(paths)\n",
        "        img = cv2.imread(image_name)[:,:,::-1]\n",
        "        \n",
        "        category_type = image_name.split('/')[-4:-3:][0]\n",
        "        defected_type = image_name.split('/')[-2:-1:][0]\n",
        "        \n",
        "        plt.figure(figsize=figsize)\n",
        "        plt.imshow(img)\n",
        "        plt.title(\n",
        "            f\"Category : {category_type} and Defected Type : {defected_type} \\n {image_name}\", \n",
        "            fontdict={'fontsize': 20, 'fontweight': 'medium'}\n",
        "        )\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2a9ea55",
      "metadata": {
        "id": "b2a9ea55"
      },
      "outputs": [],
      "source": [
        "for content in os.listdir(RESULT_PATH):\n",
        "    if content == 'images':\n",
        "        full_path = glob.glob(os.path.join(RESULT_PATH, content, '**',  '*.png'), recursive=True)\n",
        "        print('Total Image ', len(full_path))\n",
        "        print(full_path[0].split('/'))\n",
        "        print(full_path[0].split('/')[-2:-1:])\n",
        "        print(full_path[0].split('/')[-4:-3:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb3c2436",
      "metadata": {
        "scrolled": false,
        "id": "bb3c2436"
      },
      "outputs": [],
      "source": [
        "visualiz(full_path, 10, is_random=True, figsize=(30, 30))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d36fbff",
      "metadata": {
        "id": "9d36fbff"
      },
      "source": [
        "## Inference on new images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_PATH = \"/content\""
      ],
      "metadata": {
        "id": "guzVLsfc9vlY"
      },
      "id": "guzVLsfc9vlY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57e7c6fa",
      "metadata": {
        "id": "57e7c6fa"
      },
      "outputs": [],
      "source": [
        "%cd {PROJECT_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "CyMgGtK4AT68"
      },
      "id": "CyMgGtK4AT68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e2ae108",
      "metadata": {
        "id": "6e2ae108"
      },
      "outputs": [],
      "source": [
        "infer_results = PROJECT_PATH + \"/infer_results\"\n",
        "infer_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c441f0e",
      "metadata": {
        "id": "1c441f0e"
      },
      "outputs": [],
      "source": [
        "# anomalies: color, bent, flip, scratch\n",
        "# images: 1 to ~20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b68857d3",
      "metadata": {
        "id": "b68857d3"
      },
      "outputs": [],
      "source": [
        "# input image\n",
        "input_img = PROJECT_PATH + \"/metal_nut/test/bent/013.png\"\n",
        "input_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61f2bab4",
      "metadata": {
        "id": "61f2bab4"
      },
      "outputs": [],
      "source": [
        "# output image\n",
        "output_img = infer_results +  \"/metal_nut/test/bent/013.png\"\n",
        "output_img"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from anomalib.config import get_configurable_parameters\n",
        "from anomalib.data.inference import InferenceDataset\n",
        "from anomalib.models import get_model\n",
        "from anomalib.utils.callbacks import get_callbacks"
      ],
      "metadata": {
        "id": "fdcnSDVAC0An"
      },
      "id": "fdcnSDVAC0An",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import ArgumentParser, Namespace\n",
        "from pathlib import Path\n",
        "\n",
        "from pytorch_lightning import Trainer\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "Q7IRkHaCEOg1"
      },
      "id": "Q7IRkHaCEOg1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer"
      ],
      "metadata": {
        "id": "iRz4C7WaGdua"
      },
      "id": "iRz4C7WaGdua",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(trainer):\n",
        "    \"\"\"Run inference.\"\"\"\n",
        "\n",
        "    #args = get_args()\n",
        "    config = get_configurable_parameters(config_path=new_yaml_path)\n",
        "    config.trainer.resume_from_checkpoint = str(trainer.checkpoint_callback.best_model_path)\n",
        "    #config.visualization.show_images = args.show\n",
        "    config.visualization.mode = \"simple\"\n",
        "    if infer_results:  # overwrite save path\n",
        "        config.visualization.save_images = True\n",
        "        config.visualization.image_save_path = infer_results\n",
        "    else:\n",
        "        config.visualization.save_images = False\n",
        "\n",
        "    model = get_model(config)\n",
        "    callbacks = get_callbacks(config)\n",
        "\n",
        "    trainer = Trainer(callbacks=callbacks, **config.trainer)\n",
        "\n",
        "    transform_config = config.dataset.transform_config.val if \"transform_config\" in config.dataset.keys() else None\n",
        "    dataset = InferenceDataset(\n",
        "        input_img, image_size=tuple(config.dataset.image_size), transform_config=transform_config\n",
        "    )\n",
        "    dataloader = DataLoader(dataset)\n",
        "    trainer.predict(model=model, dataloaders=[dataloader])"
      ],
      "metadata": {
        "id": "4zxANiRgETxu"
      },
      "id": "4zxANiRgETxu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer(trainer)"
      ],
      "metadata": {
        "id": "U-HLzovrFWIa"
      },
      "id": "U-HLzovrFWIa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5be4468c",
      "metadata": {
        "scrolled": false,
        "id": "5be4468c"
      },
      "outputs": [],
      "source": [
        "# perform inference on the sample image\n",
        "#!python -W ignore anomalib/tools/inference/lightning_inference.py \\\n",
        "#        --config {new_yaml_path} \\\n",
        "#        --weights {trainer.checkpoint_callback.best_model_path} \\\n",
        "#        --input {input_img} \\\n",
        "#        --output infer_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0998975f",
      "metadata": {
        "scrolled": false,
        "id": "0998975f"
      },
      "outputs": [],
      "source": [
        "print(input_img)\n",
        "display(Image(input_img, width=250))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c65aa4",
      "metadata": {
        "id": "89c65aa4"
      },
      "outputs": [],
      "source": [
        "display(Image(\"/content/infer_results/bent/013.png\", width=250))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd11e205",
      "metadata": {
        "id": "dd11e205"
      },
      "source": [
        "## Summary of evaluating some arquitectures"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8adf9a8d",
      "metadata": {
        "id": "8adf9a8d"
      },
      "source": [
        "```\n",
        "MODEL = 'padim'\n",
        "```\n",
        "\n",
        "```\n",
        "'image_size': 256,\n",
        "'train_batch_size': 4,\n",
        "'test_batch_size': 4,\n",
        "'max_epochs': 4,\n",
        "```\n",
        "\n",
        "```\n",
        "[{'pixel_F1Score': 0.7553240060806274,\n",
        "  'pixel_AUROC': 0.9686623215675354,\n",
        "  'image_F1Score': 0.952380895614624,\n",
        "  'image_AUROC': 0.9496578574180603}]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0bce604",
      "metadata": {
        "id": "a0bce604"
      },
      "source": [
        "```\n",
        "MODEL = 'patchcore'\n",
        "```\n",
        "\n",
        "```\n",
        "'image_size': 128,\n",
        "'train_batch_size': 1,\n",
        "'test_batch_size': 1,\n",
        "'max_epochs': 3,\n",
        "```\n",
        "\n",
        "```\n",
        "[{'pixel_F1Score': 0.8109800219535828,\n",
        "  'pixel_AUROC': 0.9827464818954468,\n",
        "  'image_F1Score': 0.9726775884628296,\n",
        "  'image_AUROC': 0.9833822250366211}]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1217608b",
      "metadata": {
        "id": "1217608b"
      },
      "source": [
        "```\n",
        "MODEL = 'stfpm'\n",
        "```\n",
        "\n",
        "```\n",
        "'image_size': 256,\n",
        "'train_batch_size': 4,\n",
        "'test_batch_size': 4,\n",
        "'max_epochs': 4,\n",
        "```\n",
        "\n",
        "```\n",
        "[{'pixel_F1Score': 0.6699215769767761,\n",
        "  'pixel_AUROC': 0.9740051627159119,\n",
        "  'image_F1Score': 0.9555555582046509,\n",
        "  'image_AUROC': 0.9838709235191345}]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56fb27cc",
      "metadata": {
        "id": "56fb27cc"
      },
      "source": [
        "```\n",
        "MODEL = 'fastflow'\n",
        "```\n",
        "\n",
        "```\n",
        "'image_size': 256,\n",
        "'train_batch_size': 4,\n",
        "'test_batch_size': 4,\n",
        "'max_epochs': 4,\n",
        "```\n",
        "\n",
        "```\n",
        "[{'pixel_F1Score': 0.7417428493499756,\n",
        "  'pixel_AUROC': 0.9636613130569458,\n",
        "  'image_F1Score': 0.9560439586639404,\n",
        "  'image_AUROC': 0.9310851097106934}]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56c11789",
      "metadata": {
        "id": "56c11789"
      },
      "source": [
        "```\n",
        "MODEL = 'reverse_distillation'\n",
        "```\n",
        "\n",
        "```\n",
        "'image_size': 256,\n",
        "'train_batch_size': 4,\n",
        "'test_batch_size': 4,\n",
        "'max_epochs': 4,\n",
        "```\n",
        "\n",
        "```\n",
        "[{'pixel_F1Score': 0.7951029539108276,\n",
        "  'pixel_AUROC': 0.980124831199646,\n",
        "  'image_F1Score': 0.9723756313323975,\n",
        "  'image_AUROC': 0.9672531485557556}]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kucFEFupHM2k"
      },
      "id": "kucFEFupHM2k",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}