{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPo4M2nvHK3EzDACBK3iNZ1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsoy/ALOCC-CVPR2018/blob/master/nelson_1425_EfficientAD_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/nelson1425/EfficientAD"
      ],
      "metadata": {
        "id": "xts2XF3mtRS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/nelson1425/EfficientAD\n"
      ],
      "metadata": {
        "id": "3XARGGIZtT8p"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/danielsoy/EfficientAD-1.git"
      ],
      "metadata": {
        "id": "OneXt_SOu86l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b9f2d6-39f8-41ef-fdac-b17b1121575f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EfficientAD-1'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 48 (delta 12), reused 5 (delta 5), pack-reused 24\u001b[K\n",
            "Unpacking objects: 100% (48/48), 37.98 MiB | 7.35 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZZQ-ENpX_vo",
        "outputId": "f112b9fd-aaa8-496a-d0bb-33168d67af85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/EfficientAD-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udfg-HBLypYa",
        "outputId": "3af06bc9-c4df-49a8-dd19-ee4708976171"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EfficientAD-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python efficientad.py -a /content/drive/MyDrive/mvtec_anomaly_detection  --subdataset bottle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozWo0Pe1ErlB",
        "outputId": "66e8210b-05b6-4ab2-88b6-f3d1240ce933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Computing mean of features: 100% 188/188 [00:19<00:00,  9.85it/s]\n",
            "Computing std of features: 100% 188/188 [00:10<00:00, 18.53it/s]\n",
            "Current loss: 1.4027  :  14% 10000/70000 [25:36<2:29:41,  6.68it/s]\n",
            "Intermediate map normalization:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "Intermediate map normalization:  10% 2/21 [00:00<00:01, 17.92it/s]\u001b[A\n",
            "Intermediate map normalization:  24% 5/21 [00:00<00:00, 19.37it/s]\u001b[A\n",
            "Intermediate map normalization:  33% 7/21 [00:00<00:00, 19.58it/s]\u001b[A\n",
            "Intermediate map normalization:  43% 9/21 [00:00<00:00, 19.65it/s]\u001b[A\n",
            "Intermediate map normalization:  57% 12/21 [00:00<00:00, 19.83it/s]\u001b[A\n",
            "Intermediate map normalization:  67% 14/21 [00:00<00:00, 19.72it/s]\u001b[A\n",
            "Intermediate map normalization:  76% 16/21 [00:00<00:00, 19.75it/s]\u001b[A\n",
            "Intermediate map normalization:  86% 18/21 [00:00<00:00, 19.82it/s]\u001b[A\n",
            "Intermediate map normalization: 100% 21/21 [00:01<00:00, 19.75it/s]\n",
            "\n",
            "Intermediate inference:   0% 0/83 [00:00<?, ?it/s]\u001b[A\n",
            "Intermediate inference:   1% 1/83 [00:00<01:12,  1.13it/s]\u001b[A\n",
            "Intermediate inference:   2% 2/83 [00:01<01:01,  1.32it/s]\u001b[A\n",
            "Intermediate inference:   4% 3/83 [00:02<01:06,  1.20it/s]\u001b[A\n",
            "Intermediate inference:   5% 4/83 [00:03<01:08,  1.15it/s]\u001b[A\n",
            "Intermediate inference:   6% 5/83 [00:04<01:08,  1.13it/s]\u001b[A\n",
            "Intermediate inference:   7% 6/83 [00:05<01:08,  1.12it/s]\u001b[A\n",
            "Intermediate inference:   8% 7/83 [00:05<01:04,  1.17it/s]\u001b[A\n",
            "Intermediate inference:  10% 8/83 [00:06<01:05,  1.15it/s]\u001b[A\n",
            "Intermediate inference:  11% 9/83 [00:07<01:05,  1.14it/s]\u001b[A\n",
            "Intermediate inference:  12% 10/83 [00:08<01:05,  1.12it/s]\u001b[A\n",
            "Intermediate inference:  13% 11/83 [00:09<01:05,  1.11it/s]\u001b[A\n",
            "Intermediate inference:  14% 12/83 [00:10<01:05,  1.09it/s]\u001b[A\n",
            "Intermediate inference:  16% 13/83 [00:11<00:59,  1.18it/s]\u001b[A\n",
            "Intermediate inference:  17% 14/83 [00:12<00:56,  1.23it/s]\u001b[A\n",
            "Intermediate inference:  18% 15/83 [00:12<00:56,  1.20it/s]\u001b[A\n",
            "Intermediate inference:  19% 16/83 [00:13<00:54,  1.24it/s]\u001b[A\n",
            "Intermediate inference:  20% 17/83 [00:14<00:55,  1.18it/s]\u001b[A\n",
            "Intermediate inference:  22% 18/83 [00:15<00:56,  1.15it/s]\u001b[A\n",
            "Intermediate inference:  23% 19/83 [00:16<00:59,  1.08it/s]\u001b[A\n",
            "Intermediate inference:  24% 20/83 [00:17<00:58,  1.08it/s]\u001b[A\n",
            "Intermediate inference:  25% 21/83 [00:18<00:59,  1.05it/s]\u001b[A\n",
            "Intermediate inference:  27% 22/83 [00:19<00:59,  1.02it/s]\u001b[A\n",
            "Intermediate inference:  28% 23/83 [00:20<00:53,  1.12it/s]\u001b[A\n",
            "Intermediate inference:  29% 24/83 [00:21<00:54,  1.09it/s]\u001b[A\n",
            "Intermediate inference:  30% 25/83 [00:22<00:54,  1.07it/s]\u001b[A\n",
            "Intermediate inference:  31% 26/83 [00:23<00:54,  1.05it/s]\u001b[A\n",
            "Intermediate inference:  33% 27/83 [00:23<00:49,  1.12it/s]\u001b[A\n",
            "Intermediate inference:  34% 28/83 [00:24<00:45,  1.21it/s]\u001b[A\n",
            "Intermediate inference:  35% 29/83 [00:25<00:45,  1.18it/s]\u001b[A\n",
            "Intermediate inference:  36% 30/83 [00:26<00:42,  1.24it/s]\u001b[A\n",
            "Intermediate inference:  37% 31/83 [00:27<00:44,  1.18it/s]\u001b[A\n",
            "Intermediate inference:  39% 32/83 [00:28<00:45,  1.12it/s]\u001b[A\n",
            "Intermediate inference:  40% 33/83 [00:29<00:45,  1.10it/s]\u001b[A\n",
            "Intermediate inference:  41% 34/83 [00:29<00:41,  1.18it/s]\u001b[A\n",
            "Intermediate inference:  42% 35/83 [00:30<00:39,  1.22it/s]\u001b[A\n",
            "Intermediate inference:  43% 36/83 [00:31<00:36,  1.28it/s]\u001b[A\n",
            "Intermediate inference:  45% 37/83 [00:32<00:37,  1.22it/s]\u001b[A\n",
            "Intermediate inference:  46% 38/83 [00:32<00:37,  1.21it/s]\u001b[A\n",
            "Intermediate inference:  47% 39/83 [00:33<00:35,  1.25it/s]\u001b[A\n",
            "Intermediate inference:  48% 40/83 [00:34<00:33,  1.29it/s]\u001b[A\n",
            "Intermediate inference:  49% 41/83 [00:35<00:34,  1.21it/s]\u001b[A\n",
            "Intermediate inference:  51% 42/83 [00:36<00:32,  1.27it/s]\u001b[A\n",
            "Intermediate inference:  52% 43/83 [00:36<00:32,  1.22it/s]\u001b[A\n",
            "Intermediate inference:  53% 44/83 [00:37<00:30,  1.27it/s]\u001b[A\n",
            "Intermediate inference:  54% 45/83 [00:38<00:31,  1.20it/s]\u001b[A\n",
            "Intermediate inference:  55% 46/83 [00:39<00:31,  1.18it/s]\u001b[A\n",
            "Intermediate inference:  57% 47/83 [00:40<00:31,  1.15it/s]\u001b[A\n",
            "Intermediate inference:  58% 48/83 [00:41<00:31,  1.12it/s]\u001b[A\n",
            "Intermediate inference:  59% 49/83 [00:42<00:28,  1.17it/s]\u001b[A\n",
            "Intermediate inference:  60% 50/83 [00:43<00:28,  1.14it/s]\u001b[A\n",
            "Intermediate inference:  61% 51/83 [00:44<00:29,  1.07it/s]\u001b[A\n",
            "Intermediate inference:  63% 52/83 [00:45<00:28,  1.08it/s]\u001b[A\n",
            "Intermediate inference:  64% 53/83 [00:46<00:28,  1.06it/s]\u001b[A\n",
            "Intermediate inference:  65% 54/83 [00:46<00:25,  1.14it/s]\u001b[A\n",
            "Intermediate inference:  66% 55/83 [00:47<00:22,  1.24it/s]\u001b[A\n",
            "Intermediate inference:  67% 56/83 [00:48<00:22,  1.20it/s]\u001b[A\n",
            "Intermediate inference:  69% 57/83 [00:48<00:20,  1.27it/s]\u001b[A\n",
            "Intermediate inference:  70% 58/83 [00:49<00:19,  1.30it/s]\u001b[A\n",
            "Intermediate inference:  71% 59/83 [00:50<00:17,  1.37it/s]\u001b[A\n",
            "Intermediate inference:  72% 60/83 [00:51<00:16,  1.39it/s]\u001b[A\n",
            "Intermediate inference:  73% 61/83 [00:51<00:15,  1.41it/s]\u001b[A\n",
            "Intermediate inference:  75% 62/83 [00:52<00:14,  1.42it/s]\u001b[A\n",
            "Intermediate inference:  76% 63/83 [00:53<00:14,  1.41it/s]\u001b[A\n",
            "Intermediate inference:  77% 64/83 [00:54<00:14,  1.32it/s]\u001b[A\n",
            "Intermediate inference:  78% 65/83 [00:54<00:13,  1.37it/s]\u001b[A\n",
            "Intermediate inference:  80% 66/83 [00:55<00:13,  1.28it/s]\u001b[A\n",
            "Intermediate inference:  81% 67/83 [00:56<00:13,  1.22it/s]\u001b[A\n",
            "Intermediate inference:  82% 68/83 [00:57<00:12,  1.20it/s]\u001b[A\n",
            "Intermediate inference:  83% 69/83 [00:58<00:11,  1.26it/s]\u001b[A\n",
            "Intermediate inference:  84% 70/83 [00:58<00:09,  1.31it/s]\u001b[A\n",
            "Intermediate inference:  86% 71/83 [00:59<00:08,  1.34it/s]\u001b[A\n",
            "Intermediate inference:  87% 72/83 [01:00<00:08,  1.28it/s]\u001b[A\n",
            "Intermediate inference:  88% 73/83 [01:01<00:08,  1.19it/s]\u001b[A\n",
            "Intermediate inference:  89% 74/83 [01:01<00:07,  1.27it/s]\u001b[A\n",
            "Intermediate inference:  90% 75/83 [01:02<00:06,  1.31it/s]\u001b[A\n",
            "Intermediate inference:  92% 76/83 [01:03<00:05,  1.31it/s]\u001b[A\n",
            "Intermediate inference:  93% 77/83 [01:04<00:04,  1.21it/s]\u001b[A\n",
            "Intermediate inference:  94% 78/83 [01:05<00:04,  1.24it/s]\u001b[A\n",
            "Intermediate inference:  95% 79/83 [01:06<00:03,  1.14it/s]\u001b[A\n",
            "Intermediate inference:  96% 80/83 [01:06<00:02,  1.17it/s]\u001b[A\n",
            "Intermediate inference:  98% 81/83 [01:07<00:01,  1.22it/s]\u001b[A\n",
            "Intermediate inference:  99% 82/83 [01:08<00:00,  1.14it/s]\u001b[A\n",
            "Intermediate inference: 100% 83/83 [01:09<00:00,  1.20it/s]\n",
            "Intermediate image auc: 99.9206\n",
            "Current loss: 1.4732  :  15% 10152/70000 [27:09<2:27:43,  6.75it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Current loss: 1.0951  :  17% 11583/70000 [30:49<2:24:18,  6.75it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "inhT5Q3PN9Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python efficientad.py -a /content/drive/MyDrive/mvtec_anomaly_detection  --subdataset hazelnut"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e8210b-05b6-4ab2-88b6-f3d1240ce933",
        "id": "4zOXBMcVN-5-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Computing mean of features: 100% 188/188 [00:19<00:00,  9.85it/s]\n",
            "Computing std of features: 100% 188/188 [00:10<00:00, 18.53it/s]\n",
            "Current loss: 1.4027  :  14% 10000/70000 [25:36<2:29:41,  6.68it/s]\n",
            "Intermediate map normalization:   0% 0/21 [00:00<?, ?it/s]\u001b[A\n",
            "Intermediate map normalization:  10% 2/21 [00:00<00:01, 17.92it/s]\u001b[A\n",
            "Intermediate map normalization:  24% 5/21 [00:00<00:00, 19.37it/s]\u001b[A\n",
            "Intermediate map normalization:  33% 7/21 [00:00<00:00, 19.58it/s]\u001b[A\n",
            "Intermediate map normalization:  43% 9/21 [00:00<00:00, 19.65it/s]\u001b[A\n",
            "Intermediate map normalization:  57% 12/21 [00:00<00:00, 19.83it/s]\u001b[A\n",
            "Intermediate map normalization:  67% 14/21 [00:00<00:00, 19.72it/s]\u001b[A\n",
            "Intermediate map normalization:  76% 16/21 [00:00<00:00, 19.75it/s]\u001b[A\n",
            "Intermediate map normalization:  86% 18/21 [00:00<00:00, 19.82it/s]\u001b[A\n",
            "Intermediate map normalization: 100% 21/21 [00:01<00:00, 19.75it/s]\n",
            "\n",
            "Intermediate inference:   0% 0/83 [00:00<?, ?it/s]\u001b[A\n",
            "Intermediate inference:   1% 1/83 [00:00<01:12,  1.13it/s]\u001b[A\n",
            "Intermediate inference:   2% 2/83 [00:01<01:01,  1.32it/s]\u001b[A\n",
            "Intermediate inference:   4% 3/83 [00:02<01:06,  1.20it/s]\u001b[A\n",
            "Intermediate inference:   5% 4/83 [00:03<01:08,  1.15it/s]\u001b[A\n",
            "Intermediate inference:   6% 5/83 [00:04<01:08,  1.13it/s]\u001b[A\n",
            "Intermediate inference:   7% 6/83 [00:05<01:08,  1.12it/s]\u001b[A\n",
            "Intermediate inference:   8% 7/83 [00:05<01:04,  1.17it/s]\u001b[A\n",
            "Intermediate inference:  10% 8/83 [00:06<01:05,  1.15it/s]\u001b[A\n",
            "Intermediate inference:  11% 9/83 [00:07<01:05,  1.14it/s]\u001b[A\n",
            "Intermediate inference:  12% 10/83 [00:08<01:05,  1.12it/s]\u001b[A\n",
            "Intermediate inference:  13% 11/83 [00:09<01:05,  1.11it/s]\u001b[A\n",
            "Intermediate inference:  14% 12/83 [00:10<01:05,  1.09it/s]\u001b[A\n",
            "Intermediate inference:  16% 13/83 [00:11<00:59,  1.18it/s]\u001b[A\n",
            "Intermediate inference:  17% 14/83 [00:12<00:56,  1.23it/s]\u001b[A\n",
            "Intermediate inference:  18% 15/83 [00:12<00:56,  1.20it/s]\u001b[A\n",
            "Intermediate inference:  19% 16/83 [00:13<00:54,  1.24it/s]\u001b[A\n",
            "Intermediate inference:  20% 17/83 [00:14<00:55,  1.18it/s]\u001b[A\n",
            "Intermediate inference:  22% 18/83 [00:15<00:56,  1.15it/s]\u001b[A\n",
            "Intermediate inference:  23% 19/83 [00:16<00:59,  1.08it/s]\u001b[A\n",
            "Intermediate inference:  24% 20/83 [00:17<00:58,  1.08it/s]\u001b[A\n",
            "Intermediate inference:  25% 21/83 [00:18<00:59,  1.05it/s]\u001b[A\n",
            "Intermediate inference:  27% 22/83 [00:19<00:59,  1.02it/s]\u001b[A\n",
            "Intermediate inference:  28% 23/83 [00:20<00:53,  1.12it/s]\u001b[A\n",
            "Intermediate inference:  29% 24/83 [00:21<00:54,  1.09it/s]\u001b[A\n",
            "Intermediate inference:  30% 25/83 [00:22<00:54,  1.07it/s]\u001b[A\n",
            "Intermediate inference:  31% 26/83 [00:23<00:54,  1.05it/s]\u001b[A\n",
            "Intermediate inference:  33% 27/83 [00:23<00:49,  1.12it/s]\u001b[A\n",
            "Intermediate inference:  34% 28/83 [00:24<00:45,  1.21it/s]\u001b[A\n",
            "Intermediate inference:  35% 29/83 [00:25<00:45,  1.18it/s]\u001b[A\n",
            "Intermediate inference:  36% 30/83 [00:26<00:42,  1.24it/s]\u001b[A\n",
            "Intermediate inference:  37% 31/83 [00:27<00:44,  1.18it/s]\u001b[A\n",
            "Intermediate inference:  39% 32/83 [00:28<00:45,  1.12it/s]\u001b[A\n",
            "Intermediate inference:  40% 33/83 [00:29<00:45,  1.10it/s]\u001b[A\n",
            "Intermediate inference:  41% 34/83 [00:29<00:41,  1.18it/s]\u001b[A\n",
            "Intermediate inference:  42% 35/83 [00:30<00:39,  1.22it/s]\u001b[A\n",
            "Intermediate inference:  43% 36/83 [00:31<00:36,  1.28it/s]\u001b[A\n",
            "Intermediate inference:  45% 37/83 [00:32<00:37,  1.22it/s]\u001b[A\n",
            "Intermediate inference:  46% 38/83 [00:32<00:37,  1.21it/s]\u001b[A\n",
            "Intermediate inference:  47% 39/83 [00:33<00:35,  1.25it/s]\u001b[A\n",
            "Intermediate inference:  48% 40/83 [00:34<00:33,  1.29it/s]\u001b[A\n",
            "Intermediate inference:  49% 41/83 [00:35<00:34,  1.21it/s]\u001b[A\n",
            "Intermediate inference:  51% 42/83 [00:36<00:32,  1.27it/s]\u001b[A\n",
            "Intermediate inference:  52% 43/83 [00:36<00:32,  1.22it/s]\u001b[A\n",
            "Intermediate inference:  53% 44/83 [00:37<00:30,  1.27it/s]\u001b[A\n",
            "Intermediate inference:  54% 45/83 [00:38<00:31,  1.20it/s]\u001b[A\n",
            "Intermediate inference:  55% 46/83 [00:39<00:31,  1.18it/s]\u001b[A\n",
            "Intermediate inference:  57% 47/83 [00:40<00:31,  1.15it/s]\u001b[A\n",
            "Intermediate inference:  58% 48/83 [00:41<00:31,  1.12it/s]\u001b[A\n",
            "Intermediate inference:  59% 49/83 [00:42<00:28,  1.17it/s]\u001b[A\n",
            "Intermediate inference:  60% 50/83 [00:43<00:28,  1.14it/s]\u001b[A\n",
            "Intermediate inference:  61% 51/83 [00:44<00:29,  1.07it/s]\u001b[A\n",
            "Intermediate inference:  63% 52/83 [00:45<00:28,  1.08it/s]\u001b[A\n",
            "Intermediate inference:  64% 53/83 [00:46<00:28,  1.06it/s]\u001b[A\n",
            "Intermediate inference:  65% 54/83 [00:46<00:25,  1.14it/s]\u001b[A\n",
            "Intermediate inference:  66% 55/83 [00:47<00:22,  1.24it/s]\u001b[A\n",
            "Intermediate inference:  67% 56/83 [00:48<00:22,  1.20it/s]\u001b[A\n",
            "Intermediate inference:  69% 57/83 [00:48<00:20,  1.27it/s]\u001b[A\n",
            "Intermediate inference:  70% 58/83 [00:49<00:19,  1.30it/s]\u001b[A\n",
            "Intermediate inference:  71% 59/83 [00:50<00:17,  1.37it/s]\u001b[A\n",
            "Intermediate inference:  72% 60/83 [00:51<00:16,  1.39it/s]\u001b[A\n",
            "Intermediate inference:  73% 61/83 [00:51<00:15,  1.41it/s]\u001b[A\n",
            "Intermediate inference:  75% 62/83 [00:52<00:14,  1.42it/s]\u001b[A\n",
            "Intermediate inference:  76% 63/83 [00:53<00:14,  1.41it/s]\u001b[A\n",
            "Intermediate inference:  77% 64/83 [00:54<00:14,  1.32it/s]\u001b[A\n",
            "Intermediate inference:  78% 65/83 [00:54<00:13,  1.37it/s]\u001b[A\n",
            "Intermediate inference:  80% 66/83 [00:55<00:13,  1.28it/s]\u001b[A\n",
            "Intermediate inference:  81% 67/83 [00:56<00:13,  1.22it/s]\u001b[A\n",
            "Intermediate inference:  82% 68/83 [00:57<00:12,  1.20it/s]\u001b[A\n",
            "Intermediate inference:  83% 69/83 [00:58<00:11,  1.26it/s]\u001b[A\n",
            "Intermediate inference:  84% 70/83 [00:58<00:09,  1.31it/s]\u001b[A\n",
            "Intermediate inference:  86% 71/83 [00:59<00:08,  1.34it/s]\u001b[A\n",
            "Intermediate inference:  87% 72/83 [01:00<00:08,  1.28it/s]\u001b[A\n",
            "Intermediate inference:  88% 73/83 [01:01<00:08,  1.19it/s]\u001b[A\n",
            "Intermediate inference:  89% 74/83 [01:01<00:07,  1.27it/s]\u001b[A\n",
            "Intermediate inference:  90% 75/83 [01:02<00:06,  1.31it/s]\u001b[A\n",
            "Intermediate inference:  92% 76/83 [01:03<00:05,  1.31it/s]\u001b[A\n",
            "Intermediate inference:  93% 77/83 [01:04<00:04,  1.21it/s]\u001b[A\n",
            "Intermediate inference:  94% 78/83 [01:05<00:04,  1.24it/s]\u001b[A\n",
            "Intermediate inference:  95% 79/83 [01:06<00:03,  1.14it/s]\u001b[A\n",
            "Intermediate inference:  96% 80/83 [01:06<00:02,  1.17it/s]\u001b[A\n",
            "Intermediate inference:  98% 81/83 [01:07<00:01,  1.22it/s]\u001b[A\n",
            "Intermediate inference:  99% 82/83 [01:08<00:00,  1.14it/s]\u001b[A\n",
            "Intermediate inference: 100% 83/83 [01:09<00:00,  1.20it/s]\n",
            "Intermediate image auc: 99.9206\n",
            "Current loss: 1.4732  :  15% 10152/70000 [27:09<2:27:43,  6.75it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Current loss: 1.2403  :  16% 11340/70000 [30:12<2:25:32,  6.72it/s]"
          ]
        }
      ]
    }
  ]
}