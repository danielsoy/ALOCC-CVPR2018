{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsoy/ALOCC-CVPR2018/blob/master/alibinetOD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMmftWsoNT9e"
      },
      "source": [
        "https://youtu.be/Pql6ShORpNU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so2Yjw_kNT9g"
      },
      "source": [
        "\n",
        "<br>\n",
        "Outlier detection using alibi-detect<br>\n",
        "Alibi Detect is an open source Python library focused on outlier, adversarial and drift detection. <br>\n",
        "The package aims to cover both online and offline detectors for tabular data, text, <br>\n",
        "images and time series. The outlier detection methods should allow the user to <br>\n",
        "identify global, contextual and collective outliers.<br>\n",
        "pip install alibi-detect<br>\n",
        "https://github.com/SeldonIO/alibi-detect<br>\n",
        "Documentation: https://docs.seldon.io/_/downloads/alibi-detect/en/v0.5.1/pdf/<br>\n",
        "We will be using VAE based outlier detection. Based on this paper:<br>\n",
        "    https://arxiv.org/pdf/1312.6114.pdf<br>\n",
        "    <br>\n",
        "The Variational Auto-Encoder (VAE) outlier detector is first trained on a batch <br>\n",
        "of unlabeled, but normal (inlier) data. Unsupervised training is desireable since <br>\n",
        "labeled data is often scarce. The VAE detector tries to reconstruct the input it <br>\n",
        "receives. If the input data cannot be reconstructed well, the reconstruction error <br>\n",
        "is high and the data can be flagged as an outlier. The reconstruction error is either <br>\n",
        "measured as the mean squared error (MSE) between the input and the reconstructed instance <br>\n",
        "or as the probability that both the input and the reconstructed instance are <br>\n",
        "generated by the same process.<br>\n",
        "Data set info: https://openaccess.thecvf.com/content_CVPR_2019/papers/Bergmann_MVTec_AD_--_A_Comprehensive_Real-World_Dataset_for_Unsupervised_Anomaly_CVPR_2019_paper.pdf<br>\n",
        "Data set link: https://www.mvtec.com/company/research/datasets/mvtec-ad<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FyzVm4tnNT9h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_yXQi67NT9j",
        "outputId": "b48aee63-b79e-4f00-d076-fc869a39a22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SeldonIO/alibi-detect.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rD_cH7MKR478",
        "outputId": "98b8c16a-799f-4e9b-b863-39c53755359d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'alibi-detect' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install alibi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d12tf8jGTHL1",
        "outputId": "cf1e118b-1761-4062-8f82-988f704555b5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting alibi\n",
            "  Downloading alibi-0.6.5-py3-none-any.whl (398 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 40 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 81 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 398 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from alibi) (1.21.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn<1.1.0,>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from alibi) (1.0.2)\n",
            "Requirement already satisfied: scikit-image!=0.17.1,<0.20,>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from alibi) (0.18.3)\n",
            "Requirement already satisfied: tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (2.8.0)\n",
            "Requirement already satisfied: Pillow<10.0,>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from alibi) (7.1.2)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (3.2.2)\n",
            "Collecting transformers<5.0.0,>=4.7.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 45.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy[lookups]<4.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (2.2.4)\n",
            "Requirement already satisfied: attrs<22.0.0,>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from alibi) (3.10.0.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (1.4.1)\n",
            "Requirement already satisfied: pandas<2.0.0,>=0.23.3 in /usr/local/lib/python3.7/dist-packages (from alibi) (1.3.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.28.1 in /usr/local/lib/python3.7/dist-packages (from alibi) (4.63.0)\n",
            "Requirement already satisfied: dill<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from alibi) (0.3.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (1.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=0.23.3->alibi) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib<4.0.0,>=3.0.0->alibi) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi) (2021.10.8)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi) (1.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.1.0,>=0.20.2->alibi) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.1.0,>=0.20.2->alibi) (1.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (0.9.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (2.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.0.0)\n",
            "Collecting spacy-lookups-data<0.2.0,>=0.0.5\n",
            "  Downloading spacy_lookups_data-0.1.0.tar.gz (28.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.0 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy[lookups]<4.0.0,>=2.0.0->alibi) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy[lookups]<4.0.0,>=2.0.0->alibi) (3.7.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.14.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 41.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.6.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (13.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.1.2)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (3.17.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.24.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.44.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (3.3.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (3.2.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.7.0->alibi) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.7.0->alibi) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 39.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.7.0->alibi) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 47.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 30.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.7.0->alibi) (7.1.2)\n",
            "Building wheels for collected packages: spacy-lookups-data\n",
            "  Building wheel for spacy-lookups-data (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-lookups-data: filename=spacy_lookups_data-0.1.0-py2.py3-none-any.whl size=28052158 sha256=b3978d2e3341a70a7f54ee59c71763ecd0b2e9ddfa09bf85762bf81451a3ec4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/09/83/36dd0224ce32dcdf5e218b36362235ca2e50cece60a966ae1b\n",
            "Successfully built spacy-lookups-data\n",
            "Installing collected packages: pyyaml, tokenizers, tf-estimator-nightly, spacy-lookups-data, sacremoses, huggingface-hub, transformers, alibi\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed alibi-0.6.5 huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 spacy-lookups-data-0.1.0 tf-estimator-nightly-2.8.0.dev2021122109 tokenizers-0.11.6 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "smbjaerHNT9j"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Reshape, InputLayer, Flatten"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/alibi-detect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUFQasJJSao9",
        "outputId": "b9a44729-9072-414b-dfbd-10047b164f7d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/alibi-detect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ceHHPOpDNT9k"
      },
      "outputs": [],
      "source": [
        "from alibi_detect.od import OutlierAE, OutlierVAE\n",
        "from alibi_detect.utils.visualize import plot_instance_score, plot_feature_outlier_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U-_4bAVNT9k"
      },
      "source": [
        "########################################################################<br>\n",
        "oad data. We only need good data and anything NOT good is an outlier. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n"
      ],
      "metadata": {
        "id": "xHVVV_zCT6b9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9cCEQCoLamk",
        "outputId": "4ce2f424-cb63-4d1a-fd8d-5ae3d06f36c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Wb5lhRd5NT9k"
      },
      "outputs": [],
      "source": [
        "image_directory = '/content/drive/MyDrive/carpet/train/'\n",
        "SIZE = 64\n",
        "dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KUrGuS3oNT9k"
      },
      "outputs": [],
      "source": [
        "good_images = os.listdir(image_directory + 'good/')\n",
        "for i, image_name in enumerate(good_images):\n",
        "    if (image_name.split('.')[1] == 'png'):\n",
        "        image = cv2.imread(image_directory + 'good/' + image_name)\n",
        "        image = Image.fromarray(image, 'RGB')\n",
        "        image = image.resize((SIZE, SIZE))\n",
        "        dataset.append(np.array(image))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0ThKRHwoNT9l"
      },
      "outputs": [],
      "source": [
        "dataset = np.array(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "aLqOw3E8NT9l"
      },
      "outputs": [],
      "source": [
        "train = dataset[0:200]\n",
        "test = dataset[200:279]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "MTLDlVUGNT9l"
      },
      "outputs": [],
      "source": [
        "train = train.astype('float32') / 255.\n",
        "test = test.astype('float32') / 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8iIrwbpNT9m"
      },
      "source": [
        "et us also load bad images to verify our trained model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "UMq4Vzu1NT9m"
      },
      "outputs": [],
      "source": [
        "bad_images = os.listdir(image_directory + 'bad')\n",
        "bad_dataset=[]\n",
        "for i, image_name in enumerate(bad_images):\n",
        "    if (image_name.split('.')[1] == 'png'):\n",
        "        image = cv2.imread(image_directory + 'bad/' + image_name)\n",
        "        image = Image.fromarray(image, 'RGB')\n",
        "        image = image.resize((SIZE, SIZE))\n",
        "        bad_dataset.append(np.array(image))\n",
        "bad_dataset = np.array(bad_dataset)\n",
        "bad_dataset = bad_dataset.astype('float32') / 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-qGseI2NT9m"
      },
      "source": [
        "#######################################################################<br>\n",
        "efine the encoder - decoder network for input to the OutlierVAE detector class. <br>\n",
        "an be any encoder and decoder. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "pHr5tyNCNT9m"
      },
      "outputs": [],
      "source": [
        "encoding_dim = 1024  #Dimension of the bottleneck encoder vector. \n",
        "dense_dim = [8, 8, 512] #Dimension of the last conv. output. This is used to work our way back in the decoder. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0dXov9HNT9n"
      },
      "source": [
        "efine encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "wNchdaFmNT9n"
      },
      "outputs": [],
      "source": [
        "encoder_net = tf.keras.Sequential(\n",
        "  [\n",
        "      InputLayer(input_shape=train[0].shape),\n",
        "      Conv2D(64, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
        "      Conv2D(128, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
        "      Conv2D(512, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
        "      Flatten(),\n",
        "      Dense(encoding_dim,)\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOeo4xJjNT9n",
        "outputId": "11d6e0a2-0ca9-4743-a2d7-910ed20375f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        3136      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 128)       131200    \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 512)         1049088   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              33555456  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 34,738,880\n",
            "Trainable params: 34,738,880\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(encoder_net.summary())\n",
        "#print(encoder_net.input_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-DG2AMTNT9n"
      },
      "source": [
        "efine the decoder. <br>\n",
        "tart with the bottleneck dimension (encoder vector) and connect to dense layer <br>\n",
        "ith dim = total nodes in the last conv. in the encoder. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "xJ2WTL1iNT9n"
      },
      "outputs": [],
      "source": [
        "decoder_net = tf.keras.Sequential(\n",
        "  [\n",
        "      InputLayer(input_shape=(encoding_dim,)),\n",
        "      Dense(np.prod(dense_dim)),\n",
        "      Reshape(target_shape=dense_dim),\n",
        "      Conv2DTranspose(256, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
        "      Conv2DTranspose(64, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
        "      Conv2DTranspose(3, 4, strides=2, padding='same', activation='sigmoid')\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-edSGSokNT9o",
        "outputId": "a6a76758-e02b-46e3-85b7-b724913f5d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 32768)             33587200  \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 512)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 16, 16, 256)      2097408   \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 64)       262208    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 64, 64, 3)        3075      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 35,949,891\n",
            "Trainable params: 35,949,891\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(decoder_net.summary())\n",
        "#print(decoder_net.input_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8gChNT6NT9o"
      },
      "source": [
        "#####################################################################<br>\n",
        "efine and train the outlier detector. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "NGeb6s0RNT9o"
      },
      "outputs": [],
      "source": [
        "latent_dim = 1024  #(Same as encoding dim. )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tDBrcdJNT9o"
      },
      "source": [
        "initialize outlier detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "rzN7UcHGNT9o"
      },
      "outputs": [],
      "source": [
        "od = OutlierVAE(threshold=.015,  # threshold for outlier score above which the element is flagged as an outlier.\n",
        "                score_type='mse',  # use MSE of reconstruction error for outlier detection\n",
        "                encoder_net=encoder_net,  # can also pass VAE model instead\n",
        "                decoder_net=decoder_net,  # of separate encoder and decoder\n",
        "                latent_dim=latent_dim,\n",
        "                samples=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD7FbfhGNT9p",
        "outputId": "c424d38c-517e-44e4-8724-a1fc3cf297f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current threshold value is:  0.015\n"
          ]
        }
      ],
      "source": [
        "print(\"Current threshold value is: \", od.threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BadWsGJUNT9p"
      },
      "source": [
        "train<br>\n",
        "rom alibi_detect.models.tensorflow.losses import elbo #evidence lower bound loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsVPLQstNT9p"
      },
      "outputs": [],
      "source": [
        "adam = tf.keras.optimizers.Adam(lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bguIxvZwNT9p",
        "outputId": "8ca23688-ef7b-4c72-96c2-0d3642279c7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/50 [.] - ETA: 46s - loss_ma: 23550.0054"
          ]
        }
      ],
      "source": [
        "od.fit(train,\n",
        "       optimizer = adam,\n",
        "       epochs=20,\n",
        "       batch_size=4,\n",
        "       verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn3V6alWNT9p"
      },
      "source": [
        "heck the threshold value. Should be the same as defined before. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3ANxCyXNT9p"
      },
      "outputs": [],
      "source": [
        "print(\"Current threshold value is: \", od.threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-G-1_fKNT9q"
      },
      "source": [
        "<br>\n",
        "nfer_threshold Updates threshold by a value inferred from the percentage of <br>\n",
        "nstances considered to be outliers in a sample of the dataset.<br>\n",
        "ercentage of X considered to be normal based on the outlier score.<br>\n",
        "ere, we set it to 99%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boOWYIBNNT9q"
      },
      "outputs": [],
      "source": [
        "od.infer_threshold(test, outlier_type='instance', threshold_perc=99.0)\n",
        "print(\"Current threshold value is: \", od.threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEa1GvTBNT9q"
      },
      "source": [
        "save the trained outlier detector<br>\n",
        "s mentioned in their documentation, save and load is having issues in python3.6 but works fine in 3.7<br>\n",
        "rom alibi_detect.utils import save_detector, load_detector<br>\n",
        "ave_detector(od, \"saved_outlier_models/carpet_od_20epochs.h5\")<br>\n",
        "d = load_detector(filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ampt2DiZNT9q"
      },
      "source": [
        "est our model on a bad image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuvadR3dNT9q"
      },
      "outputs": [],
      "source": [
        "img_num = 9\n",
        "test_bad_image = bad_dataset[img_num].reshape(1, 64, 64, 3)\n",
        "plt.imshow(test_bad_image[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq1HkeatNT9r"
      },
      "outputs": [],
      "source": [
        "test_bad_image_recon = od.vae(test_bad_image)\n",
        "test_bad_image_recon = test_bad_image_recon.numpy()\n",
        "plt.imshow(test_bad_image_recon[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLO7EXAMNT9r"
      },
      "outputs": [],
      "source": [
        "test_bad_image_predict = od.predict(test_bad_image) #Returns a dictionary of data and metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SazKGeM3NT9r"
      },
      "source": [
        "ata dictionary contains the instance_score, feature_score, and whether it is an outlier or not. <br>\n",
        "et u look at the values under the 'data' key in our output dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0elwf9DNT9r"
      },
      "outputs": [],
      "source": [
        "bad_image_instance_score = test_bad_image_predict['data']['instance_score'][0]\n",
        "print(\"The instance score is:\", bad_image_instance_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KufaDMmfNT9r"
      },
      "outputs": [],
      "source": [
        "bad_image_feature_score = test_bad_image_predict['data']['feature_score'][0]\n",
        "plt.imshow(bad_image_feature_score[:,:,0])\n",
        "print(\"Is this image an outlier (0 for NO and 1 for YES)?\", test_bad_image_predict['data']['is_outlier'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkxjUrN4NT9r"
      },
      "source": [
        "ou can also manually define the threshold based on your specific use case. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwEPXK9CNT9r"
      },
      "outputs": [],
      "source": [
        "od.threshold = 0.002\n",
        "print(\"Current threshld value is: \", od.threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmCiIQuBNT9s"
      },
      "source": [
        "et us check it for multiple images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyurkNvdNT9s"
      },
      "outputs": [],
      "source": [
        "X = bad_dataset[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPeiGFiBNT9s"
      },
      "outputs": [],
      "source": [
        "od_preds = od.predict(X,\n",
        "                      outlier_type='instance',    # use 'feature' or 'instance' level\n",
        "                      return_feature_score=True,  # scores used to determine outliers\n",
        "                      return_instance_score=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4wRfKinNT9s"
      },
      "outputs": [],
      "source": [
        "print(list(od_preds['data'].keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53Jc5bBlNT9s"
      },
      "source": [
        "catter plot of instance scores. using the built-in function for the scatterplot. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypSwycnkNT9s"
      },
      "outputs": [],
      "source": [
        "target = np.ones(X.shape[0],).astype(int)  # Ground truth (all ones for bad images)\n",
        "labels = ['normal', 'outlier']\n",
        "plot_instance_score(od_preds, target, labels, od.threshold) #pred, target, labels, threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE9WNTtcNT9s"
      },
      "source": [
        "lot features for select images, using the built in function (plot_feature_outlier_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6FJarQrNT9s"
      },
      "outputs": [],
      "source": [
        "X_recon = od.vae(X).numpy()\n",
        "plot_feature_outlier_image(od_preds,\n",
        "                           X,\n",
        "                           X_recon=X_recon,\n",
        "                           instance_ids=[0, 5, 10, 15, 17],  # pass a list with indices of instances to display\n",
        "                           max_instances=5,  # max nb of instances to display\n",
        "                           outliers_only=False)  # only show outlier predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCEk9TuoNT9t"
      },
      "source": [
        "#####################################"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "alibinetOD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}